[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "L'Œuvre",
    "section": "",
    "text": "Explore product management, data science, & the metaverse.\n\n\n\n\n\n\n“Turn yourself not away from three best things: Good Thought, Good Word, and Good Deed.” ~ Zarathushtra\nThe website content is divided into four main types:\n\nProjects: Longer term and more complex work involving code, and other software websites.\nArticles: Shorter term work involving coding and analysis and opinion. Reviews would come under articles.\nGuides: Longer term and frequently updated notes for learning topics.\nMisc: Random stuff that does not fit anything formal.\n\nThe content categories are further divided into four main areas:\n\nProduct Management: Planning, creating, designing, managing, & selling products. UI/UX design, technology, business & startups.\nData Science: Data science, analytics, visualization, machine learning & artificial intelligence.\nMetaverse: Includes Augmented, Virtual, Mixed, Extended Reality hardware, software, code & business\nSoftware: Usage, coding, setup, scripts, algorithsm, & recommendations.\n\nClick here to view Portfolio of work"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "L'Œuvre",
    "section": "",
    "text": "Explore product management, data science, & the metaverse.\n\n\n\n\n\n\n\n\n\nAbout this website.\nCurrent Work & Projects:\n\nReview of paper “A comprehensive overview of software product management challenges”\nJira Product Discovery Workflows\nExploring Inclusive Design for new HCI through the metaverse\n\nBelow is a list of all work done and ongoing projects.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPost With Code\n\n\nFeb 17, 2023\n\n\n\n\nWelcome To My Blog\n\n\nFeb 17, 2023\n\n\n\n\nSQL & Database Administration Setup & Tips\n\n\nMar 26, 2023\n\n\n\n\nSQL Guide.\n\n\nFeb 18, 2020\n\n\n\n\nSome questions to ask entrepreneurs\n\n\nFeb 11, 2016\n\n\n\n\nExtracting initial parameters from an existing Holt‐Winter forecasting model\n\n\nNov 29, 2015\n\n\n\n\nForecasting with seasonal trends at BLAYK restaurant\n\n\nNov 24, 2015\n\n\n\n\nChoice of moving average or exponential smoothing for a particular product profile\n\n\nNov 23, 2015\n\n\n\n\nForecasting for Sugar Bon‐Bon Cereals\n\n\nNov 23, 2015\n\n\n\n\nExponential smoothing models at TrainMax Systems\n\n\nNov 23, 2015\n\n\n\n\nEvaluation of stationary demand models\n\n\nNov 20, 2015\n\n\n\n\nAnalysis of three black‐box type demand forecasting models\n\n\nNov 19, 2015\n\n\n\n\nSuitability of stationary demand models for forecasting\n\n\nNov 18, 2015\n\n\n\n\nPerformance characteristics of forecasting models\n\n\nNov 18, 2015\n\n\n\n\nImproving the naïve model forecast using cumulative period model\n\n\nNov 18, 2015\n\n\n\n\nHow do I to get started in & learn ‘data science’, ‘data analytics’?\n\n\nFeb 21, 2014\n\n\n\n\nThe best options to study for SAS Certified Base Programmer for SAS 9 Credential\n\n\nFeb 21, 2014\n\n\n\n\nCombinatorial analysis & calculations using SAS functions – fact(), perm() and comb()\n\n\nMar 7, 2014\n\n\n\n\nHow to find out and use the number of observations in a given SAS data set\n\n\nMar 16, 2014\n\n\n\n\nReferring to a SAS data set with its full filesystem path\n\n\nApr 4, 2014\n\n\n\n\nSoftware choices to implement a remote database client/server network setup\n\n\nMay 8, 2014\n\n\n\n\nProject to implement remote PHP web client/ MSSQL database server network setup\n\n\nMay 16, 2014\n\n\n\n\nConnect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC\n\n\nMay 18, 2014\n\n\n\n\nWhat is ‘data science’ / data analytics? - Yet another opinion\n\n\nFeb 21, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "p2p.html#product-strategy-template",
    "href": "p2p.html#product-strategy-template",
    "title": "Path 2 Product",
    "section": "Product Strategy Template:",
    "text": "Product Strategy Template:\nPhase 1: title\nGoal: the primary goal. Challenge: most immediate challenge to make progress as an org. First to attack first. Guiding policy: constraints place on ourselves to focus & alignment to overcome above challenge. Coherent Actions: initiatives to do within the constraints. Expected Outcomes: goal & outcome needs to be in alignment.\nOKR driven company.\nRest of course is inside community.\nPhase 2:"
  },
  {
    "objectID": "posts/01-whatisdatascience/index.html",
    "href": "posts/01-whatisdatascience/index.html",
    "title": "What is ‘data science’ / data analytics? - Yet another opinion",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:52 am\nIn the last twenty years of the Information Age, computer networks & the Internet have allowed us to gather & store information from a wide variety of devices (industrial/medical equipment, photos/audio/video sensors, ATMs, credit/debit cards, mobiles & computers, etc..) & platforms (retailing, telecom, banking & insurance , pharmaceutical, security, social media, etc..).\nThe information being collected is increasing as more people & businesses use computers & the Internet as a primary means for conducting business, social & monetary transactions. Technologies such as 3G/4G data networks, mobile computing devices & cloud based computing systems have accelerated this trend.\n\n“The value of data is no longer in how much of it you have. In the new regime, the value is in how quickly and how effectively can the data be reduced, explored, manipulated and managed.”\nUsama Fayyad – President & CEO of digiMine, Inc. [1]\n\nQuestions to think about:\n\nCan the increasing amount of information be collected, stored & managed in a consistent manner for easy access to others?\nCan information from different sources be integrated & linked to create meaningful relationships between them?\nCan information systems be built to easily get answers to characteristics of stored data?\nCan we detect underlying patterns in the data & “mine” the data to reveal patterns of behavior that will be insightful & profitable in terms of business/research before it loses its value?\nCan analytic processes/models be built to allow prediction of future outcomes/behavior from existing data more quickly than the competition?\nCan it be ensured that business/research returns will be greater than the investment cost of the data collection & analytics system?\n\n\nIt takes brilliance to ask the right questions, at the right time in history. The value of a Big Data resource is that a good analyst can start to see connections between different types of data, and this may prompt the analyst to determine whether there is a way to describe these connections in terms of general relationships among the data objects [2]\n\nIt is important to realize that most of these problems have been discussed [1] & studied in research journals & other industry publications for the last 30-40 years under various labels such as “Business Intelligence”, “Knowledge Discovery”, “Data Mining”, “Decision Science”, “Statistical Learning”, “Predictive Modeling” “Machine Learning”, “Business Forecasting”, etc… Essentially, it is the coming together of data analysis techniques, large scale computing & domain knowledge.\nNow in the present time, all these are under the labels of “Big Data” [2] “Data Science” & “Data Analytics”. The big change now is in the commoditization of technologies where all these techniques can be applied in a cost effective manner & almost in real-time.\nWhile it is not realistically possible for a single person to perform all the above tasks, a new practitioner has emerged who has the relevant knowledge in statistical & data mining techniques, computing & programming techniques as well as domain knowledge of the business/research problem.\nThe main skill required (besides the usual technical knowledge) is curiosity about what patterns exist or can be “mined” from stored data sources as well what can be predicted from it, the ability to experiment with new methods to get new insights & explanations just like a scientist would. Perhaps that is why they call people in this field “data scientists”!\nMy own equation for this “emerging” field would be:\n\nData Science/Data Analytics =\nDomain knowledge of the business/research problem +\nMathematical formulation of business/research problem into a statistical model +\nProgramming the statistical model into software code +\nBusiness/Research analysis of the statistical output\n\nReferences\n[1] HAMPARSUM BOZDOGAN (ed.) Statistical data mining and knowledge discovery (2003) Chapman & Hall/CRC\n[2] JULES J BERMAN Principles of big data: preparing, sharing, and analyzing complex information (2013) Morgan Kaufmann"
  },
  {
    "objectID": "posts/02-howdoigetstarted/index.html",
    "href": "posts/02-howdoigetstarted/index.html",
    "title": "How do I to get started in & learn ‘data science’, ‘data analytics’?",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:57 am\nThis is an open ended question that has created a lot of discussion on the Web. There are no right answers or approaches. It all depends on what you are interested in & want to achieve. If you choose the self study route, this would be my personal approach.\n1) Decide your domain of interest: The fields of “data analytics”, “data science” are very vast in their scope to learn everything. So although the general statistical & analytic principles are the same for all fields, it is best to find your industry of interest (say pharmaceutical, econometrics, finance, social & bio sciences, human resources, actuarial sciences, marketing, energy forecasting, predictive modeling, business statistical analysis, medical sciences, etc..) & develop domain specific knowledge for your interests. This will allow you to focus your learning & effort. Trying to master too many domains might get you confused & mentally drained in the long run.\n2) Application or technical side: This is a broad generalization. Application side is where the understanding & application is more important than the implementation of the code or the software system. Typically for students of business, social sciences, pharmaceutical, etc…\nUsually understanding & framing the research/business problem required to be solved precedes analysis. The important thing is to know the business impact of your analysis. For example, by changing the values of the variables or doing a “WHAT…IF” analysis, one should be able to interpret the change in output in terms of how it addresses the research/business problem that is required to be solved. This type of skill comes only with having the right domain knowledge.\nTechnical side usually interests students of mathematics, computer science, engineering & fields where focus is on developing new techniques, software & improving existing ones. The fields of data mining & “Big Data” are pretty technical in terms of the mathematical & programming knowledge required. This includes algorithms & equations for data mining, machine learning, pattern recognition, text processing. Database design using “Big Data” technologies like Hadoop, NOSQL, Hive, MongoDb, PIG, MapReduce etc… Also most existing databases like MS SQL Server & Oracle have data mining features.\nAs technical conferences & statistical journals discuss the latest techniques & methods in terms of mathematics, it would be best to get the mathematical background (usually calculus 1 & 2, linear algebra & probability at undergraduate level) as quickly as you can. This will not only enable you to understand but also express yourself in terms of mathematics.\nThe ability to understand the research/business problem & convert it into mathematical form & then choose or create an appropriate algorithmic method, that is relatively fast & with minimum error, for the specific software system are skills you should aim to acquire.\n3) Books & reading material: The books & material you read from should match your level of expertise & also must be based on the software you plan to use. Your best option is to use academic websites, publisher websites & book review websites like Amazon.com to know the contents & the subject matter of the books. There are specialized statistical books for students of social sciences, marketing, computer science, pharmaceutical etc… Also books that teach data analytics/data science/statistics using particular software (such as R, SAS, SPSS, STATA, MS Excel, Python & many more).\nBuy textbooks on the following criteria:\n\nBased on your mathematical level i.e. based on advanced math like calculus or simple math like algebra.\nThat teaches & uses the software you plan to learn with. Using R, SAS, SPSS, STATA, MS Excel, etc….\nThat deal with the techniques & methods for your domain of interest i.e. finance, marketing, pharmaceutical, biostatistics etc…\n\n4) Get the software running: Being familiar with the various functions & features of the software is almost as important as learning statistical theory. You want to be productive & not waste time looking up help/documentation all the time.\nThere are software specifically for statistical analysis. Some are suited for certain domains & industries. The hyperlink below shows a list of them.\nhttp://www.amstat.org/careers/statisticalsoftware.cfm\nThe R project software is the ideal to begin learning with. It is extensive & has many user submitted packages for almost every kind of statistical analysis. It is available for free (as in gratis).\nSince R is distributed under the GPL software license you might need to be familiar with the licensing issues of the various R packages especially if you plan to use R commercially & your code also contains other proprietary code using restrictive licenses.\nMicrosoft Excel is a good option as most office/college computers have machines running MS Windows & MS Office. It has many add-ins (e.g. XLMiner, neuroXL, Oracle Spreadsheet Add-In, Perfringens Predictor Excel Add-in, ADAPA Add-in for Microsoft® Office Excel®, RExcel, DataMinerXL, SAS Add-in for Rapid Predictive Modeler, Palisade Neuraltools Add-in, 11Ants Model Builder Microsoft Excel Add-in, etc.. ) available for analytics & data mining. It can also be used to interact with the data mining features of MS SQL Server.\nCommercial statistical software like SAS, IBM SPSS & STATA are used widely in industry & academia. Those in academia should be able to get an academic license to access & use SAS/SPSS/STATA on their personal computers. SAS also offers an SAS OnDemand service to access SAS through the Internet for a fee.\n5) Programming: Since each software package has its own programming environment. Learning a general programming course at undergraduate level will help you understand programming principles that each software uses. Those technically inclined would do well to do a course in computer algorithms & database design.\nKnowing SQL is important since most of the data you will analyze or process will reside in a database system like MySQL, MS SQL Server, Oracle etc…\nA major part of analytics & data science is modifying existing data into a particular format (especially dates, currency, telephone, number formats) for processing. Every software has built-in features for checking errors & missing values, replacing, searching, sorting, filtering & extracting text from a larger dataset. Text processing tools like grep, perl, and python are also used.\nIt is good idea, although not necessary, to get the basic certifications for commercial software like SAS, SPSS. The exams allow you to brush up your skills & your clients or the company would be somewhat assured of your competency.\nLast but not least, it is best to explore websites & join academic/industry communities specific to your area of research or choice.\nSummary:\n\nDecide on your area of interest & domain.\nDo you prefer to be on the technical/programming side or be application/business oriented?\nGet the right books & study material for your academic level & area of interest.\nDecide on the appropriate software used by your industry.\nLearn programming & algorithms.\nExplore the Internet for websites & join communities specific to your needs."
  },
  {
    "objectID": "posts/03-thebestwaytostudyforsas/index.html",
    "href": "posts/03-thebestwaytostudyforsas/index.html",
    "title": "The best options to study for SAS Certified Base Programmer for SAS 9 Credential",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:59 am\nIf SAS software will be a part of your career then get certified by passing the SAS Base Programming Exam from SAS Institute. It is designed to test your knowledge in writing SAS programs to access & manage data to perform queries and analyses. It also creates a good impression to your client / company.\nhttp://support.sas.com/certify/creds/bp.html\nOption 1: If you can afford it or have your company/institution sponsor/subsidize you, then the two courses offered by the SAS Institute is the best way to learn the Base SAS system & programming.\nSAS Programming 1: Essentials http://support.sas.com/edu/schedules.html?id=277&ctry=US\nSAS Programming 2: Data Manipulation Techniques http://support.sas.com/edu/schedules.html?id=278&ctry=US\nIt is a highly structured course with code examples, exercises & practice questions that will thoroughly teach you the basic concepts of SAS software & Base SAS programming.\nOption 2: The next best option is to purchase the following book\nBase SAS Programmer Certification 3rd edition\nhttp://www.amazon.com/SAS-Certification-Prep-Guide-Programming/dp/1607649241/ref=sr_1_1?ie=UTF8\nIt is a condensed version of the full course & hence is the best option for those that cannot afford the full course. Use it with the Base SAS software available at your academic institution or company & practice all the example code. You can also buy access to SAS OnDemand to use SAS software through the Internet to practice for the exam.\nOption 3: Join an institute that offer training in SAS & statistical analysis. They usually frame their SAS course syllabus & material from the official courses & have computer labs with access to Base SAS software. The advantage here is having an instructor who can give you feedback on your progress & clear your doubts & queries regarding the exam.\nPreparation: You can be ready with 4-5 months of preparation if you follow any of the above options. Modifying existing code & using different procedures / keywords to achieve the same result is best way to understand how SAS works. It requires time & practice as SAS software is vast & comprehensive.\nSAS Institute also offers a practice exam that you can purchase to test yourself before attempting the actual exam.\nIt should be understood that in addition to all this, there is an abundant amount of material on the Web to help you learn Base SAS programming & prepare for the certification credential exam. All the best!"
  },
  {
    "objectID": "posts/04-combinatorialfunctionsas/index.html",
    "href": "posts/04-combinatorialfunctionsas/index.html",
    "title": "Combinatorial analysis & calculations using SAS functions – fact(), perm() and comb()",
    "section": "",
    "text": "Created on Friday, March 7th, 2014 at 5:56 am\nSAS provides several kinds of functions for doing combinatorial analysis and calculations. Three basic ones will be demonstrated here. fact(),perm() and comb(). All three functions return a missing value for the arguments they cannot compute.\nfact(n) is the function for calculating the factorial n! of any non-negative number n.\n275  data _null_;\n276  a=fact(-3);\n277  b=fact(0);\n278  c=fact(9);\n279  d=fact(170);*170! is the max calculable by this particular computer.;\n280  e=fact(171);\n281  f=fact(1000);\n282  put _all_;\n283  run;\nThe output is given by:\nNOTE: Invalid argument to function FACT at line 276 column 3.\nNOTE: Invalid argument to function FACT at line 280 column 3.\nNOTE: Invalid argument to function FACT at line 281 column 3.\na=. b=1 c=362880 d=7.257416E306 e=. f=. _ERROR_=1 _N_=1\na=. b=1 c=362880 d=7.257416E306 e=. f=. _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places. The results of the\n      operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 276:3   1 at 280:3   1 at 281:3\nNOTE: DATA statement used (Total process time):\nSimilarly for permutation of n objects taken r at a time (where n&gt;r), we have the perm(n,r). A single argument in the perm() function will calculate the factorial of the argument.\n384  data _null_;\n385  a=perm(3,3);\n386  b=perm(2,5);\n387  c=perm(5,2);\n388  d=perm(5,4);\n389  e=perm(1660,170);\n390  f=perm(4);\n391  put _all_;\n392  run;\nThe output is given by\nNOTE: Argument 2 to function PERM at line 386 column 3 is invalid.\nNOTE: Invalid argument to function PERM at line 389 column 3.\na=6 b=. c=20 d=120 e=. f=24 _ERROR_=1 _N_=1\na=6 b=. c=20 d=120 e=. f=24 _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places. The results of the\n      operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 386:3   1 at 389:3\nNOTE: DATA statement used (Total process time):\nSimilarly for combination of n objects taken r at a time (where n&gt;r), we have the comb(n,r). The comb() function requires two arguments.\n433  data _null_;\n434  a=comb(3,3);\n435  b=comb(2,5);\n436  c=comb(5,2);\n437  d=comb(5,4);\n438  e=comb(9960,170);\n439  f=comb(1,0);\n440  put _all_;\n441  run;\nThe output is given by:\nNOTE: Argument 2 to function COMB at line 435 column 3 is invalid.\nNOTE: Invalid argument to function COMB at line 438 column 3.\na=1 b=. c=10 d=5 e=. f=1 _ERROR_=1 _N_=1\na=1 b=. c=10 d=5 e=. f=1 _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places.\n      The results of the operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 435:3   1 at 438:3\nWe have the logarithmic (natural) counterparts of the above three functions i.e. lfact(), lperm() and lcomb() The output is given below.\n452  data _null_;\n453  a=lfact(10);\n454  b=lperm(10,5);\n455  c=lcomb(10,5);\n456  put _all_;\n457  run;\n\na=15.104412573 b=10.31692083 c=5.5294290875 _ERROR_=0 _N_=1\nNOTE: DATA statement used (Total process time):\n      real time           0.00 seconds\n      cpu time            0.00 seconds"
  },
  {
    "objectID": "posts/05-howtofindnoofobssas/index.html",
    "href": "posts/05-howtofindnoofobssas/index.html",
    "title": "How to find out and use the number of observations in a given SAS data set",
    "section": "",
    "text": "Created on Sunday, March 16th, 2014 at 5:43 am\nIf one is in a situation where they have to know the number of observations in a particular SAS data set or use it for further calculations, SAS allows many ways of doing so. Let us look at a few quick ways using the DATA step. SAS documentation also includes a SAS Macro approach which will not be replicated here.\nMethod 1: The most straightforward way is to load the set in a _null_ DATA step.\n data _null_;\n set dataset3obs;\n run;\nThe output is given by\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 2: Another way to extract & print is to output the NOBS= value, but since the DATA step loops to read all observations, the put statement is executed at every iteration.\n data _null_;\n set dataset3obs nobs=nobs;\n put nobs;\n run;\nThe output is given by\n 3\n 3\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 3: One way to overcome this & print a single value is to output the number of observations either at the beginning or at the end of DATA step.\n data _null_;\n set dataset3obs nobs=nobs;\n if _n_=1 then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 4: Output at the end of the data loop.\n data _null_;\n set dataset3obs nobs=nobs end=last;\n if last then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 5: One can even extract the value before the SET statement, as SAS loads it in the descriptor portion during the compilation phase.\n data _null_;\n set dataset3obs nobs=nobs;\n if _n_=1 then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nThere are many other methods but these are simple for just a quick lookup."
  },
  {
    "objectID": "posts/06-referringsasdatasetfullpath/index.html",
    "href": "posts/06-referringsasdatasetfullpath/index.html",
    "title": "Referring to a SAS data set with its full filesystem path",
    "section": "",
    "text": "Posted on Friday, April 4th, 2014 at 3:14 am\nThe standard way of referring to a SAS data set is through its library reference. i.e. LIBREF.DATASETNAME. However SAS also allows accessing the data set using the full filesystem path. Although not very useful, it does show the versatility that SAS allows the user. It can be used in situations where the data set needs to be accessed without defining a library name reference to access it.\nproc print data=sasuser.admit;\nrun;\nUsing the full filesystem path we get:\nproc print data='C:\\Documents and Settings\\sasuser\\My Documents\\My SAS Files\\9.1\\admit.sas7bdat'; \nrun;"
  },
  {
    "objectID": "posts/07-softwarechoicesremotedb/index.html",
    "href": "posts/07-softwarechoicesremotedb/index.html",
    "title": "Software choices to implement a remote database client/server network setup",
    "section": "",
    "text": "Posted on Thursday, May 8th, 2014 at 2:25 am\nInteracting with a database on a network from statistical/reporting software like MS Excel, MS Access, SAS, R, Python, etc… is a great way to learn how data might be retrieved, analysed & stored remotely. Web scripting languages like ASPX, PHP also allow remote database interactions for data analysis using Web based applications.\nHowever, a local database with local software using only individual local files pretty much provides the same experience as using them on a network. But the aim here is to simulate a corporate/research environment where all the software & data resources are spread throughout the network. The aim is not to become an expert in the technologies but to know just enough to use them to do data science/analytics.\nHence the philosophy is to use minimum hardware/software resources to be able to study & learn quickly and efficiently. The problem is that each person has a different configuration of client/server and hardware/software components & usually one person’s solution will not necessarily work for the other. The choices available to you will be different from the choices presented here.\nServer decision process:\n\nServer database: MS SQL Server (MSSQL) vs. Mysql Database (mysql)\nDecision: MSSQL\nReason: MSSQL has built-in data mining through SQL Server Analysis Services (SSAS). Also MSSQL2008 offers more data mining options when compared to MSSQL2005. There is more information online about MSSQL data mining features than about mysql data mining features. Furthermore, MS Excel can perform data mining directly on spreadsheets using SSAS via an add-in\nServer OS: Windows XP (winxp) vs. Windows Vista/7 (win7)\n\nDecision: win7\n\nReason: win7 has similar features to Windows Server 2008 & handles networking better than winxp. Winxp would be sufficient but one needs to install additional software components such as dotnet packages, VC++ run-time packages, deal with permissions/networking issues etc…\nOnce you have the server & software setup for networking & remote access, almost any hardware/software combination can be used as a client.\nClient decision process:\n\nClient OS: Windows vs. GNU/Linux\n\nDecision: Windows\nReason: MS Excel and MS Access are used widely in the business/research world. Hence Windows XP was chosen. Also MS Excel supports many add-ins for statistical analysis, data mining & visualization. Although software such as R, Python, WEKA etc… would integrate better with GNU/Linux.\n\nClient web server: MS Internet Information Services (IIS) vs. Apache Web Server (apache).\n\nDecision: IIS\n\nReason: A student of data science/analytics does not need advanced features of a web server. A basic version of IIS comes built-in with winxp. I am guessing it is enough for the purpose of learning. Apache is robust but requires more configuration than IIS.\n\nClient web language: ASPX vs. PHP\n\nDecision:PHP\n\nReason: PHP is easy to configure for IIS & light on system resources. ASPX would be the preferred choice for use with IIS and MSSQL, but requires more configuration & systems resources for Visual Studio 2010 development environment. Students can look at the PHPStats project https://github.com/mcordingley/PHPStats for using statistical functions to do data analysis on the web.\n\nOS for Web Server: Client OS vs. Server OS\n\nDecision: Client OS\n\nReason: Decision to install IIS/PHP on Client OS was primarily done for two reasons i) To isolate the database from any instability that might result from the Web application environment ii) To simulate an “Internet” where the database & web server reside on different machines.\n\nMisc. client software: MS Excel, MS Access, R, Python. It is advisable to try the database access features of many software for learning & practice.\nSummary:\nFinal server setup: SQL Server 2008 on Windows 7.\nFinal client setup: PHP using IIS on Windows XP.\nThe main idea is to simulate a corporate/research data science/analytics environment where data & software resources are spread throughout the network. The design choices are made to use minimum resources. This enables students to learn the basics efficiently without having to worry about the advanced features. The implementation of these choices will be discussed in another post."
  },
  {
    "objectID": "posts/08-projecttoimplementremotedb/index.html",
    "href": "posts/08-projecttoimplementremotedb/index.html",
    "title": "Project to implement remote PHP web client/ MSSQL database server network setup",
    "section": "",
    "text": "Posted on Friday, May 16th, 2014 at 2:43 pm\nFor students learning data science/analytics, accessing data sets stored from databases in remote servers is a necessary skill. People in corporations/institutions have Systems Administrators to set up most of the interfaces for such purposes. However knowing how such a process is setup is a good knowledge. Here PHP is used to demonstrate database access.\nAlthough this project is not related to data science/analytics, it will give a good hands-on experience of client/server networking. These are mere guidelines and not details as specific options & settings will depend on the version of the software & components.\nPre-requisites: Fairly good understanding of computer hardware especially networking, Windows operating system, drivers & software installation. Hardware/Networking\nPhysical: Atleast two computers with Ethernet/WiFi capability and a wireless/wired router.\nFinal Setup: Ability to create Workgroups & shared folders. Also ability to ping & telnet the remote computers/servers. Server Setup\nThere are many versions & editions MSSQL depending on user requirements (The Developer Edition is best for learning & academic use). For data mining features, SQL Server Analysis Services(SSAS) must be installed. To install OLTP/DW AdventureWorks sample databases, enable FILESTREAM and make sure the SQL Full-text Filter Daemon Launcher service is running.\n\nIf you have the space, do a full install of every feature. It will make it easier to troubleshoot problems.\nCreate a MSSQL user exclusively for remote access. http://technet.microsoft.com/en-us/library/aa337545.aspx\nAllow remote connection to MSSQL. http://technet.microsoft.com/en-us/library/ms191464.aspx\nEnable Shared Memory, TCP/IP, pipes http://technet.microsoft.com/en-us/library/ms181035.aspx\nIn the Windows Firewall, allow incoming/outgoing connections for port 1433 (default MSSQL port.) http://blogs.msdn.com/b/walzenbach/archive/2010/04/14/how-to-enable-remote-connections-in-sql-server-2008.aspx\n\nFinal Setup: From the Command Prompt in the client, execute ping dbpcip and telnet dbpcip 1433 where dbpcip is the IP address of MSSQL machine. If you can connect with both the commands, the client/server networking is probably working. Although accessing the datebase & tables finally depends on the permissions assigned to the remote user. Client Setup\nKeep these details in mind before starting installation. 1) Some of the types of PHP functions for access to MSSQL. This article only discusses mssql_() and sqldrv_().\n`mssql_()` Supported by PHP 5.2 (php52) and lower. MSSQL Driver for PHP (SSDPHP) not required\n`sqldrv_()` Supported by PHP 5.3 (php53) and higher. SSDPHP required.\n`pdo_sqlsrv_()` Supported by php52 and higher. SSDPHP required.\n`pdo_odbc_()` Supported by php51 and higher. SSDPHP not required.\n\nSQL Server Native Client (SSNC): This is needed by PHP to access MSSQL Server. SSNC2008: installs on winxp SSNC2012: does not install on winxp. php52: requires atleast SSNC2008 php53: requires atleast SSNC2012\nProblem: php53 onwards requires SSNC2012, but SSNC2012 does not work on winxp. Also php53 does not support mssql_() functions.\n\nSolution: SSNC2008 works on winxp. Use php 5.2.13 to 5.2.17 as php52 requires SSNC2008. Also php 5.2.x supports mssql_()functions and sqldrv_() functions if you install the SSDPHP 2.0. Client software\n\nInternet Information Services (IIS): Official Documentation: https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/iiiisin2.mspx?mfr=true\n\nStep-by-step illustrated guide: Follow until step 10 http://www.wikihow.com/Configure-IIS-for-Windows-XP-Pro\nYou can test if it is working by opening the http://localhost/ address on the web browser. You should also test it on a remote computer using http://clientip/\nFinal Setup: Windows Version: XP IIS Version: 5.1\n\nFastCGI:\n\nFastCGI helps IIS work better with PHP. Install the one which is compatible with your version of IIS & OS. Official source: http://www.iis.net/downloads/microsoft/fastcgi-for-iis\nFinal Setup:fcgisetup_1.5_rtw_x86.msi\n\nPHP installation:\n\n\nSteps to installing PHP with IIS are explained in &lt;www.php.net/manual/en/install.windows.iis6.php&gt;\n5.2 and lower mssql_()documentation http://www.php.net/manual/en/book.mssql.php\n5.3 and higher sqlsrv_() documentation http://www.php.net/manual/en/book.sqlsrv.php\n5.2 and higher pdo_sqlsrv() documentation http://www.php.net/manual/en/ref.pdo-sqlsrv.php\n5.1 and higher pdo_odbc() documentation http://www.php.net/manual/en/ref.pdo-odbc.php\n\nFinal Setup: php-5.2.17-nts-Win32-VC6-x86.msi\n\nMSSQL Driver for PHP:\n\nOfficial Documentation: http://technet.microsoft.com/en-us/library/dn425064%28v=sql.10%29.aspx\nInstallation help: http://www.iis.net/learn/application-frameworks/install-and-configure-php-on-iis/install-the-sql-server-driver-for-php\nFinal Setup: SQLSRV20.exe\n\nSQL Server Native Client:\n\nUse this page to decide what components are needed for your setup. http://msdn.microsoft.com/en-us/library/cc296170%28SQL.105%29.aspx\nFinal Setup: sqlncli2k8r2x86.msi\n\nOverview: The entire process can be very broadly summarised as:\n\nIIS -&gt; FastCGI -&gt; PHP 5.3 -&gt; sqlsrv_() -&gt; SSDPHP 3.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; sqlsrv_() -&gt; SSDPHP 2.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; mssql_()-&gt; No SSDPHP -&gt; SSNC -&gt; MSSQL Testing the final setup:\n\nPHP System Information Test: If PHP is properly installed, the following code should execute:\n\nSOURCE CODE:\n&lt;?php echo phpinfo(); ?&gt;\nYou should get PHP system information which includes information about msql_() and sqlsrv_() extensions. Only relevant partial output shown below.\nOUTPUT:\n\n    \n        Registered PHP Streams \n            php, file, data, http, ftp, compress.zlib, compress.bzip2, \n        https, ftps, zip, sqlsrv   \n    \n\n\n\n\ncgi-fcgi\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        cgi.check_shebang_line\n            1\n            1\n    \n\n    \n        cgi.fix_pathinfo\n            1\n            1\n    \n\n    \n        cgi.force_redirect\n            0\n            0\n    \n\n    \n        cgi.nph\n            0\n            0\n    \n\n    \n        cgi.redirect_status_env\n            no value\n            no value\n    \n\n    \n        cgi.rfc2616_headers\n            0\n            0\n    \n\n    \n        fastcgi.impersonate\n            1\n            1\n    \n\n    \n        fastcgi.logging\n            0\n            0\n    \n\n\n\nmsql\n\n\n    \n        MSQL Support \n            enabled \n    \n\n    \n        Allow Persistent Links \n            yes \n    \n\n    \n        Persistent Links \n            0/unlimited \n    \n\n    \n        Total Links \n            0/unlimited \n    \n\n\n\nmssql\n\n\n    \n        MSSQL Support\n            enabled\n    \n\n    \n        Active Persistent Links \n            0 \n    \n\n    \n        Active Links \n            0 \n    \n\n    \n        Library version \n            7.0 \n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        mssql.allow_persistent\n            On\n            On\n    \n\n    \n        mssql.batchsize\n            0\n            0\n    \n\n    \n        mssql.compatability_mode\n            Off\n            Off\n    \n\n    \n        mssql.connect_timeout\n            5\n            5\n    \n\n    \n        mssql.datetimeconvert\n            On\n            On\n    \n\n    \n        mssql.max_links\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.max_persistent\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.max_procs\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.min_error_severity\n            10\n            10\n    \n\n    \n        mssql.min_message_severity\n            10\n            10\n    \n\n    \n        mssql.secure_connection\n            Off\n            Off\n    \n\n    \n        mssql.textlimit\n            Server default\n            Server default\n    \n\n    \n        mssql.textsize\n            Server default\n            Server default\n    \n\n    \n        mssql.timeout\n            60\n            60\n    \n\n\n\npdo_sqlsrv\n\n\n    \n        pdo_sqlsrv support\n            enabled\n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        pdo_sqlsrv.log_severity\n            0\n            0\n    \n\n\n\nsqlsrv\n\n\n    \n        sqlsrv support\n            enabled\n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        sqlsrv.LogSeverity\n            0\n            0\n    \n\n    \n        sqlsrv.LogSubsystems\n            0\n            0\n    \n\n    \n        sqlsrv.WarningsReturnAsErrors\n            On\n            On\n    \n\n\nRemote MSSQL Access Test: If phpinfo() indicates that all the drivers & extensions are installed properly, then test to see if the database server & client are properly registered.\n\nSOURCE CODE:\n\n&lt;?php\n$serverName = \"dbpcip, 1433\"; //serverName\\instanceName, portNumber (default is 1433)\n$connectionInfo = array( \"Database\"=&gt;\"master\", \"UID\"=&gt;\"userName\", \"PWD\"=&gt;\"passWord\");\n$conn = sqlsrv_connect( $serverName, $connectionInfo);\n\nif( $conn ) {\n     echo \"Connection established.\n\";\n}else{\n     echo \"Connection could not be established.\n\";\n     die( print_r( sqlsrv_errors(), true));\n}\n\nif( $client_info = sqlsrv_client_info( $conn)) {\n    foreach( $client_info as $key =&gt; $value) {\n        echo $key.\": \".$value.\"\n\";\n    }\n} else {\n    echo \"Error in retrieving client info.\n\";\n}\n\n\n$server_info = sqlsrv_server_info( $conn);\nif( $server_info )\n{\n    foreach( $server_info as $key =&gt; $value) {\n       echo $key.\": \".$value.\"\n\";\n    }\n} else {\n      die( print_r( sqlsrv_errors(), true));\n}\n?&gt;\nOUTPUT:\nConnection established.\n\n\n    \n        Parameter\n            Value\n    \n\n    \n        DriverDllName:\n            sqlncli10.dll\n    \n\n    \n        DriverODBCVer:\n            03.52\n    \n\n    \n        DriverVer:\n            10.50.1600\n    \n\n        \n        ExtensionVer:\n            2.0.1802.200\n    \n\n        \n        SQLServerVersion:\n            10.50.1600\n    \n\n        \n        SQLServerName:\n            DBPC\n\nTest of SQL Query: Once a connection is established, the final test is to see whether query execution is possible or not. This is a quick crude code. But the fact that it retrieves data from master.dbo.spt_monitor table shows the setup & the connection works.\n\nSOURCE CODE:\n&lt;?php\n$serverName = \"dbpcip, 1433\"; //serverName\\instanceName, portNumber (default is 1433)\n$connectionInfo = array( \"Database\"=&gt;\"master\", \"UID\"=&gt;\"userName\", \"PWD\"=&gt;\"passWord\");\n$conn = sqlsrv_connect( $serverName, $connectionInfo);\n\nerror_reporting(-1);\n\n\nif( $conn ) {\n     echo \"Connection established.\n\";\n}else{\n     echo \"Connection could not be established.\n\";\n     die( print_r( sqlsrv_errors(), true));\n}\n/* SQL Query */\n$sql=\"select * from dbo.spt_monitor\";\n$results = sqlsrv_query( $conn, $sql );\nif( $results === false) {\n    die( print_r( sqlsrv_errors(), true) );\n}\n    echo \"MSSQL master.dbo.spt_monitor TABLE\n\";\n        echo \"\n            &lt;table border=1&gt;\n            &lt;tr&gt;\n                &lt;th&gt;cpu_busy&lt;/th&gt;\n                &lt;th&gt;io_busy&lt;/th&gt;\n                &lt;th&gt;idle&lt;/th&gt;\n                &lt;th&gt;pack_received&lt;/th&gt;\n                &lt;th&gt;pack_sent&lt;/th&gt;\n                &lt;th&gt;connections&lt;/th&gt;\n                &lt;th&gt;pack_errors&lt;/th&gt;\n                &lt;th&gt;total_read&lt;/th&gt;\n                &lt;th&gt;total_write&lt;/th&gt;\n                &lt;th&gt;total_errors&lt;/th&gt;\n             &lt;/tr&gt;\";\n    while ($row = sqlsrv_fetch_array($results))\n    {\n                $cpu_busy=$row[1];\n                $io_busy=$row[2];\n                $idle=$row[3];\n                $pack_received=$row[4];\n                $pack_sent=$row[5];\n                $connections=$row[6];\n                $pack_errors=$row[7];\n                $total_read=$row[8];\n                $total_write=$row[9];\n                $total_errors=$row[10];\n             \n    echo \"\n            &lt;tr&gt;\n                &lt;td&gt;$cpu_busy&lt;/td&gt;;\n                &lt;td&gt;$io_busy&lt;/td&gt;;\n                &lt;td&gt;$idle&lt;/td&gt;;\n                &lt;td&gt;$pack_received&lt;/td&gt;;\n                &lt;td&gt;$pack_sent&lt;/td&gt;;\n                &lt;td&gt;$connections&lt;/td&gt;;\n                &lt;td&gt;$pack_errors&lt;/td&gt;;\n                &lt;td&gt;$total_read&lt;/td&gt;;\n                &lt;td&gt;$total_write&lt;/td&gt;;\n                &lt;td&gt;$total_errors&lt;/td&gt;;\n            &lt;/tr&gt;\";\n    }\n    echo \"&lt;/table&gt;\";\n    ?&gt;\nOUTPUT:\nConnection established.\n\nMSSQL master.dbo.spt_monitor TABLE\n\n; ; ; ; ; ; ; ; ; ;\n\n            \n\n                \n                    cpu_busy\n                        io_busy\n                        idle\n                        pack_received\n                        pack_sent\n                        connections\n                        pack_errors\n                        total_read\n                        total_write\n                        total_errors\n                \n\n                \n                    9\n                        7\n                        792\n                        28\n                        28\n                        14\n                        0\n                        0\n                        0\n                        0\n                \n\nIncorrect SSDPHP version:\n\nProblem: If you use php53/SSDPHP 3.0 code but use SSNC2008 instead of SSNC 2012, you get an error message.\nCould not connect. Array ( [0] =&gt; Array ( [0] =&gt; IMSSP [SQLSTATE] =&gt; IMSSP [1] =&gt; -49 [code] =&gt; -49 [2] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: http://go.microsoft.com/fwlink/?LinkId=163712 [message] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: http://go.microsoft.com/fwlink/?LinkId=163712 ) [1] =&gt; Array ( [0] =&gt; IM002 [SQLSTATE] =&gt; IM002 [1] =&gt; 0 [code] =&gt; 0 [2] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified [message] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified ) )\nSolution: Install php52/SSDPHP 2.0 & SSNC2008\nSummary:\nAlthough this project takes a lot of time to setup, troubleshoot & tweak all the settings, the end result is a client/server setup one can experiment with. Once the database server is setup properly, various types of software can be configured to retrieve/store data. PHP can be used to create web applications for doing data analysis using MSSQL.\nTo access data from MSSQL using php52, mssql_() functions is used. But php53 onwards uses only sqldrv_() functions to access MSSQL. Hence it is better to learn sqldrv_() functions. However knowing how to use the older functions will help when dealing with legacy systems having php52."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "",
    "text": "Created on Sunday, May 18th, 2014 at 3:24 am\nOne of the important skills in SAS is being able to connect & query a local or remote database, conduct data analysis in SAS & write the new information back to the database. SAS provides access to a variety of databases as well as different ways of doing so. In this article, access to Microsoft SQL Server (MSSQL) using SAS/ACCESS ODBC interface is explained. If you are using SAS at an institute or organization, your Systems Administrator would have setup everything. The steps in this article might vary depending on the software configuration."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#summary",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#summary",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "Summary:",
    "text": "Summary:\nThe SAS/ACCESS interface component is used to connect to various kinds of databases. Three main ways of connecting SAS with MSSQL are using SAS/ACCESS interface for MS SQL Server, SAS/ACCESS interface for OLE DB & SAS/ACCESS interface for ODBC. SAS allows data retrieval/storage using both LIBNAME statement and direct SQL Pass-Through statement method.\nWhat is shown above is very elementary code just to get the process started. Students learning data science/analytics should try to use different combinations of SAS methods & SAS interfaces to retrieve data from a database, manipulate it using SAS/SQL & write the processed data sets back to the database."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#references",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#references",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "References:",
    "text": "References:\n\nSAS/ACCESS 9.2 for Relational Databases Reference, Fourth Edition. http://support.sas.com/documentation/cdl/en/acreldb/63647/HTML/default/viewer.htm#titlepage.htm\n\nSAS 9.2 SQL Procedure User’s Guide. http://support.sas.com/documentation/cdl/en/sqlproc/62086/HTML/default/viewer.htm#titlepage.htm"
  },
  {
    "objectID": "posts/13-questionstoaskentrepreneurs/index.html",
    "href": "posts/13-questionstoaskentrepreneurs/index.html",
    "title": "Some questions to ask entrepreneurs",
    "section": "",
    "text": "Based on MITx bootcamp notes\n\nPart 1 - Basic questions:\nQ01. What is the problem you want to solve?\nQ02. Who experiences the problem?\nQ03. How do you want to solve this problem?\nQ04. Why is this a better solution?\nQ05. If you could describe your product in about 10 words without using anything fancy. How would you say it?\nQ06. What is the one thing you feel you can do (for your customer) better than everyone else?\n\n\nPart 2 - Business Plan:\nQ07. Why did you pick up this particular field of all the other things to solve?\nQ08. What are the many business opportunities do you see in this field?\nQ09. What skills do you need to learn to pursue these opportunities?\nQ10. And with people of what skills, strengths, and interests would you like to collaborate in that pursuit?\n\n\nPart 3 - Market Segmentation:\nQ11. What are you top 3 markets (e.g. educational, aerospace, medical etc… ) for your business?\nQ12. Did you do primary customer research? If so, what was the main feedback given?\nQ13. In what way did you change your business model as a result of this feedback?\n\n\nPart 4 - Beachhead market: initial market segment that is easy to grow & profit\nQ14. What do you think is your beachhead market? (e.g. small biz.,schools, houses, retail etc…)\nQ15. Is there competition that could block you from getting this business of this market?\nQ16. What is your plan to deal with them?\nQ17. If you win this market will it help you to win other market segments?\n\n\nPart 5 - End user profile: Common characteristics among all your customers\nQ18. Have you profiled your customers in terms of demographics, their motivations for solving their problem & also socio-economic profiles?\nQ19. Do you have the unique characteristics of your customers that you can use to identify other customers?\nQ20. How many such kind of customers are there in your beachhead market?\n\n\nPart 6 - TAM (Total addressable market): Total revenue from your beachhead market.\nTAM = Total no. of customers possible * Avg. Revenue per customer per Year.\nQ21. Do you have any idea of your TAM size? Can you capture 100% of it?\nQ22. How much percent can you achieve in the next 5 years?\n\n\nPart 7 - Persona: More detailed profiling of customers.\nQ23. How do you plan to get your next 100 customers?\n\n\nPart 8 - High Level Product Specification: Complete overview of the product.\nQ24. Does your team & customers have the same idea about what the service is & the direction it is evolving?\nQ25. What steps are you taking to improve on this?\n\n\nPart 9 - Last Words:\nQ26. What motivates you to do this every single day?\nQ27. What keeps you awake at night about your business?\nQ28. Is there any situation where you might seriously consider walking away?\nQ29. What are the top 3 things you learnt from starting & running this?\nQ30. Anything you wish to share about your experiences that most people don’t know?\n\n\nPart 10 - Introspection\n\nWhat made you interested to meet this entrepreneur and how the entrepreneur’s work or life story aligns with your interests.\nWhat you learned about the entrepreneur and the entrepreneur’s startup before your meeting, and what questions you prepared for the entrepreneur.\nWhat you learned by meeting the entrepreneur, and how that compares or contrasts with your perspective before the meeting.\nWhat problem the entrepreneur is solving, for what target customer, with what solution, and what makes the solution unique.\nDescribe your thoughts on the potential of the entrepreneur’s startup."
  },
  {
    "objectID": "posts/14-sqldbadmin/index.html",
    "href": "posts/14-sqldbadmin/index.html",
    "title": "SQL & Database Administration Setup & Tips",
    "section": "",
    "text": "r.bat File to read an sql script from the command line & display the content back to the CLI.\nusage: r.bat script_name.sql\necho off\nset arg1=%1\nset arg2=%~n1\nshift\nshift\n\nrem mysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt; &lt; %arg1%\nmysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt; &lt; %arg1%\n\n\na.bat File to read an sql script from the command line & display the content back to a text file.\nusage: a.bat script_name.sql\necho off\nset arg1=%1\nset arg2=%~n1\nshift\nshift\n\nmysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt; &lt; %arg1% &gt; %arg2%.txt\nmore %arg2%.txt\n\n\nrunserver.bat add path and start mysql client.\nusage: runserver.bat\n\n\n\nrunserver.bat add path and start mysql client.\nusage: runserver.bat\necho off\nset path=C:\\mysql-5.1-winx64\\bin;%path%\nrem sudo ifup enp0s8\nmysql -h &lt;hostname&gt; -u &lt;username&gt; -p&lt;pass&gt; &lt;dbname&gt;"
  },
  {
    "objectID": "posts/cs01-suitabilityofstationarydemandmodels/index.html",
    "href": "posts/cs01-suitabilityofstationarydemandmodels/index.html",
    "title": "Suitability of stationary demand models for forecasting",
    "section": "",
    "text": "Objective\nStudy the suitability of stationary demand models for forecasting sales at the Shah Alam Palm Oil Company.\n\n\nIntroduction\nPalm oil is harvested from the fruit of oil palm trees and is widely used as a cooking oil throughout Africa, Southeast Asia, and parts of Brazil. It is becoming widely used throughout the world as it is a lower cost alternative to other vegetable oils and has other attractive properties.\nThe Shah Alam Palm Oil Company (SAPOC) harvests, processes, and sells palm oil throughout the region. As a demand analyst, you are asked to review the sales volume (in pounds) of you premium palm oil by one of your customers, a local grocery store in the region.\n\n\nVisualization of the raw data\n\nQ1. What is the trend over the last three years?\nThere appears to be a positive trend. From the graph there is an increase in the demand of palm oil by about 24 lbs per month for the last three years.\n\nQ2. Does there appear to be any seasonality in the demand pattern?\nYes. If we plot the data by the months for each of the years, there seems to be seasonality to the demand. Demand is low from January to May. It picks up from June to August and then again from October to December.\n\nQ3. What is the forecast for demand in January 2015?\n\n\n\n\n\n\n\n\nMethod\nForecast for Jan 2015 (lbs)\nActual value in Dec 2014\n\n\n\n\nNaïve Model\n1512\n1512\n\n\nCumulative Model\n957.9444\n1512\n\n\n12 Period Moving Average\n1173.667\n1512\n\n\n\nQ4. What is the root mean square error (RMSE) for a next period forecast for these three years of demand?\n\n\n\nMethod\nRMSE\n\n\n\n\n\nNaïve Model\n383.7282\n\n\n\nCumulative\n419.8851\n\n\n\n12 Period Moving Average\n423.33\n\n\n\n\nQ5. Which of these three models is most appropriate for forecasting the January 2015 demand?\nNone. As shown above, the palm oil data shows a positive trend & seasonality during the years. The Naïve model forecast for Jan 2015 is clearly different from the previous trends. While the cumulative & naïve models are quite calm indicating they are forecasting demand closer to the average of the data. The main reason for this discrepancies is that the above three models assume a stationary demand that is very close to the level of the mean."
  },
  {
    "objectID": "posts/cs02-performancecharacteristics/index.html",
    "href": "posts/cs02-performancecharacteristics/index.html",
    "title": "Performance characteristics of forecasting models",
    "section": "",
    "text": "Objective\nInvestigate existing forecasting capabilities of Ordroid devices & provide suggestions.\n\nIntroduction\nYou have just been hired by a company that manufactures mid‐range communication devices that use the Ordroid open source operating system. The company is focused on innovating its products and has not put much thought on its inventory or forecasting capabilities. Your boss thinks there might be a problem in the forecasting of the Ordroid Devices and wants you to figure it out. The Ordroid, far from being new to the market, has been out for two years.\nKnowing this, you have asked for data on both years of historical sales as well as any forecasts, promotions, pricing changes, or competitive analyses made during this time. Your boss laughs and provides you with all the data they have: the last six months of sales. You ask to meet with the current demand planner for the Ordroid Devices and she tells you that they use a forecasting algorithm of her own design and there is no documentation.\n\n\nVisualization of the raw data\nRaw data & forecasts supplied by the demand planner at Ordroid Devices\n\nCalculate some different performance characteristics for the data sample given.\n\\[\nMD=\\frac{\\sum_{i=1}^{n} (Actual_{i}-Forecast_{i})}{n}\n\\] \\[\nMAD=\\frac{\\sum_{i=1}^{n} \\lvert Actual_{i}-Forecast_{i}\\rvert}{n}\n\\] \\[\nRMSE=\\sqrt{\\frac{\\sum_{i=1}^{n} (Actual_{i}-Forecast_{i})^2}{n}}\n\\] \\[\nMPE=\\frac{\\sum_{i=1}^{n} \\frac{ (Actual_{i}-Forecast_{i})}{Actual_{i}}}{n}\n\\] \\[\nMAPE=\\frac{\\sum_{i=1}^{n} \\lvert \\frac{Actual_{i}-Forecast_{i}}{Actual_{i}}\\rvert}{n}\n\\]\n\n\n\n\n\n\n\nNumber of devices\n\n\n\n\nMean Deviation\nMD\n112.5\n\n\nMean Absolute Deviation\nMAD\n509.5\n\n\nRoot Mean Square Error\nRMSE\n540.6115\n\n\nMean Percent Error\nMPE\n0.04112\n\n\nMean Absolute Percent Error\nMAPE\n0.269002\n\n\n\n\nQ1. What can you say about the presence seasonality of demand?\nSeasonality requires a whole cycle. There is not even one full year of data. So as of now, it is too early to fully evaluate seasonality. You need at least two full cycles to determine seasonality.\nQ2. What can you say about the presence of a trend in the demand?\nAlthough we don’t have a year’s worth of data, there seems to be a positive trend of about 10% increase in demand in the data or about 171 devices per month.\n\nQ3. What can you say about the bias of the forecast?\nA bias is a persistent tendency to over or under predict. These forecasts are not persistent in either. In fact, of the six periods, half are over forecast and half are under forecast. So, there does not appear to be any bias in the forecast.\nQ4. What can you say about the accuracy of the forecast?\nThis is not a very good forecast because even though there is a strong positive trend, the forecasts ignores the trend & also the MAPE is almost 27% ‐ quite high."
  },
  {
    "objectID": "posts/cs03-comparisonnaivecumulative/index.html",
    "href": "posts/cs03-comparisonnaivecumulative/index.html",
    "title": "Improving the naïve model forecast using cumulative period model",
    "section": "",
    "text": "Objective:\nComparing error metrics by switching naïve model to cumulative model.\nIntroduction:\nYou have been hired by General Miles, a company that produces healthy gluten‐free breakfast cereal bars. The last market introduction happened a year ago and your manager thinks there might be an issue in the forecasting methodology. They are currently using a simple Naive forecasting model and you think there might be some room for improvement.\nYour boss provides you with the sales for the last 12 months and the forecasts for the last 11 months. No data is available to forecast the first month as the product was totally new to the market at the time.\nVisualization of the raw data\n\nComparison of the forecasting models\n\nComparison of Model Error Metrics\n\n\n\nError Metric\nRMSE\nMAPE\n\n\n\n\nNaïve Model\n80.45665\n0.155729\n\n\nCumulative Model\n131.226\n0.269029\n\n\n\nConclusion:\nThe cumulative model in this case is worse than the Naïve model since the RMSE and the MAPE values are greater. It does not react quickly enough to adapt to the high variability in sales for this new gluten free cereal bar."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "",
    "text": "To help the Fenway Park concessions evaluate and compare the quality of these three competing forecasting approaches."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#visualization-of-raw-data",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#visualization-of-raw-data",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Visualization of raw data",
    "text": "Visualization of raw data"
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#comparison-of-the-mean-deviation-md-among-the-three-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#comparison-of-the-mean-deviation-md-among-the-three-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Comparison of the mean deviation (MD) among the three models",
    "text": "Comparison of the mean deviation (MD) among the three models\n\n\n\n\n\n\n\n\nModel\nMD\n% deviation from mean of actual sales (3750.056)\n\n\n\n\nM1\n‐526.988\n‐14.0528\n\n\nM2\n24.45679\n0.652171\n\n\nM3\n‐11.6852\n‐0.3116\n\n\n\nAs seen above M1 exhibits the most bias. M3 has the least bias. This is done to\nModels 2 and 3 have an average error in the vicinity of only a few dozen hotdogs from the actual. In fact,\nM2 & M3 have a MD equivalent to less than 1% of the average hot dog sales per game.\nHowever, Model 1 seems to consistently over‐estimate the demand for hotdogs by an average of 500 units per game ‐ or about 14% of the average hot dog sales per game). Therefore, we can say that – based on the historical data we have – Models 2 and 3 are less biased than Model 1. Model 1 is the most biased of the pack and it is OVER forecasting."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#calculation-of-rmse-of-each-of-the-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#calculation-of-rmse-of-each-of-the-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Calculation of RMSE of each of the models",
    "text": "Calculation of RMSE of each of the models\n\n\n\nModel\nRMSE\n\n\n\n\nM1\n597.6846\n\n\nM2\n286.457\n\n\nM3\n500.2935\n\n\n\nAs seen above, M2 has an error that is half of the other models & hence is the most suitable forecasting model besides also have a small deviation in forecasted sales from the actual demand."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#effect-of-underestimating-overestimating-by-the-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#effect-of-underestimating-overestimating-by-the-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Effect of underestimating & overestimating by the models",
    "text": "Effect of underestimating & overestimating by the models\nAs mentioned in the introduction,\n\n\n\n\n\n\n\n\nForecasting Issues\nEffect\nCost\n\n\n\n\nUnderestimating\nUnsatisfied & hungry fans\n$3 per lost sale\n\n\nOverestimating\nWaste of ingredients, energy & labor\n$2 per unsold hot dog\n\n\n\nCalculations of the lost‐sale or unsold inventory by the three models.\n\n\n\nMetric\nM1\nM2\nM3\n\n\n\n\nShortage of hotdogs\n0\n21765\n32883\n\n\nCost of Lost Sales $\n0\n65295\n98649\n\n\nUnsold Hotdogs\n85372\n17803\n34776\n\n\nCost of Unsold hotdogs $\n170744\n35606\n69552\n\n\nTotal Loss $\n170744\n100901\n168201\n\n\n\n\nLeast Total loss"
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#conclusion",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#conclusion",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Conclusion",
    "text": "Conclusion\nWhen comparing the MD, RMSE among the three models, M2 seems to be the most accurate in its predictions and it also manages to provide the least total loss & a balance between the cost of lost sales and unsold hot dogs better than the other two models.\nTherefore, from among the three available options, M2 seems to make the predictions that make most economic sense."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html",
    "href": "posts/cs05-evaluationofstationarydemand/index.html",
    "title": "Evaluation of stationary demand models",
    "section": "",
    "text": "Objective\nSelect a suitable model among the given choices."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#check-for-stationary-demand",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#check-for-stationary-demand",
    "title": "Evaluation of stationary demand models",
    "section": "Check for stationary demand",
    "text": "Check for stationary demand\nOne way of doing is to determine the coefficient of variation (CV)\n\\[\nCV=\\frac{STDEV(data)}{AVERAGE(data)}\n\\]\nWe get\n\n\n\nSTDEV\n62.6899312\n\n\n\n\nAVERAGE\n1103.78571\n\n\nCV\n0.05679538\n\n\n\nCV is very low & hence the demand is quite stationary & stable in nature."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#calculations-among-the-models",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#calculations-among-the-models",
    "title": "Evaluation of stationary demand models",
    "section": "Calculations among the models",
    "text": "Calculations among the models\n\n\n\n\nPrediction for period 15\nMAPE(%)\n\n\n\n\nPeriod 14 value\n1169\n\n\n\nNaïve\n1169\n7.08446274\n\n\nCumulative\n1103.78571\n5.13422811\n\n\n2MA\n1145\n6.46420824\n\n\n4MA\n1113.25\n4.79251584\n\n\n\nHere we see that the moving average forecasts need not always be between the naïve & cumulative forecasts."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#selection-of-the-model-in-the-presence-of-a-trend",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#selection-of-the-model-in-the-presence-of-a-trend",
    "title": "Evaluation of stationary demand models",
    "section": "Selection of the model in the presence of a trend",
    "text": "Selection of the model in the presence of a trend\n\nIf we assume there is a positive trend in the data then none of these models are appropriate for demand with a trend pattern. The Cumulative, Naive, and Moving Average forecasts all assume stationary demand. That means that you only assume a Level pattern to the demand with some random noise."
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "",
    "text": "The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)"
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html#introduction",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html#introduction",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "",
    "text": "The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)"
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html#visualization-of-raw-data",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html#visualization-of-raw-data",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "Visualization of raw data",
    "text": "Visualization of raw data\n\n\nForecasting with 5-point moving average & simple exponential smoothing\nHere we use α=1/3 for our exponential smoothing model\n\n\n\nAccuracy of the models\nTo estimate the accuracy of the models, we first compare the Mean Absolute Deviation (MAD) for each of the models.\n\n\n\n\n5MA\nSES\n\n\n\n\nMAD\n453.7429\n432.0553\n\n\n\nAlso comparing the error cumulatively gives a better picture of the accuracy of each of the models.\n\n\n\nConclusion\nFor this particular product, the forecasts from June to December show that SES is performing better than 5MA. SES outperforms 5MA for 5 months while 5MA outperforms SES for only 2 months."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "",
    "text": "Objective\nFormulation & testing of different exponential models on the product data."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#simple-exponential-smoothing-ses",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#simple-exponential-smoothing-ses",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Simple Exponential Smoothing (SES)",
    "text": "Simple Exponential Smoothing (SES)\nWe know that SES assumes stationary demand. i.e. it forecasts does not take into account trends or seasonalities. Even so, we would still like to know the effect of using SES on the forecasts.\nUnderlying model: \\(x_{t} = a + e_{t}\\)\nForecasting model: \\(\\hat{x}_{t,t+1} = \\alpha x_{t} + (1 – \\alpha) \\hat{x}_{t-1,t}\\)\nWhere \\(\\hat{x}_{t,t+1}\\) is forecast for the next period, \\(x_{t}\\) is the present actual demand and \\(\\hat{x}_{t-1,t}\\) is forecast for the previous period.\nInitialization of the parameters\nThere are many ways of doing this. We can take the centered average for the first 4 or 5 periods. We can also take the average of the first 3, 4 or 5 periods depending on the data.\nWe take \\(\\hat{a}_{4,5}\\)=205.25\nWe take α=0.15\nUsing the above model, the forecast for period 25 is around 654 bars. Also the forecast for period\n30 will also be the same i.e. 654 since the model assumes stationary demand.\nMAPE for SES is 0.279329"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#holts-model-hm",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#holts-model-hm",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Holt’s Model (HM)",
    "text": "Holt’s Model (HM)\n\nSince the data shows a positive trend, we use HM which assumes level & trend.\nUnderlying model: \\(x_{t} = a + bt + e_{t}\\)\nForecasting model: \\(\\hat{x}_{t,t+\\tau} = \\hat{a}_{t}+\\tau \\hat{b}_{t}\\)\nUpdating Component:\n\\(\\hat{a}_{t}=\\alpha \\hat{x}_{t}+ (1-\\alpha)\\hat{x}_{t-1,t}\\)\na^t = α xt + (1 – α) x^t‐1,t\n\\(\\hat{b}_{t}=\\beta (\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta) \\hat{b}_{t-1}\\)\nb^t = β (a^t ‐ a^t‐1) + (1 – β) b^t‐1\nInitialization of parameters\nWe take α = 0.2 and β = 0.05\nWe take \\(\\hat{a}_{t}\\) = 157.5 & \\(\\hat{b}_{t}\\) = 19.1"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#comparison-of-models",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#comparison-of-models",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Comparison of models",
    "text": "Comparison of models"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#mape-of-the-various-models",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#mape-of-the-various-models",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "MAPE of the various models",
    "text": "MAPE of the various models\n\n\n\n\nMAPE\n\n\n\n\nSES\n0.279329\n\n\nHM (alp=0.2bet=0.05)\n0.128729\n\n\nHM (alp=0.5bet=0.05)\n0.120443\n\n\nHM (alp=0.99bet=0.05)\n0.112559\n\n\n\nThe MAPE and various other measures such as RMSE or MAD or most any other metric will improve as we increase the value of Alpha. This does not mean we are getting a better model. This means is that we are only fitting the model better to the historical data that we have. We are simple placing more weight to the most recent observations. Therefore, we should monitor effect of alpha & change as and when required."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#conclusion",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#conclusion",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Conclusion",
    "text": "Conclusion\nHolt’s model with alpha=0.2 seems to be best way to forecast the demand of sugar cereals."
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "",
    "text": "Objective:"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#simple-exponential-smoothing-ses-model",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#simple-exponential-smoothing-ses-model",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Simple Exponential Smoothing (SES) model",
    "text": "Simple Exponential Smoothing (SES) model\nWe try this model as it looks like there is stationary demand & no trend. We need to assign the initial parameters first. We start with period 0 where we assume the forecast for the period 1 is the same as the demand for period 1. Also assume initial α=0.12\nUsing this we have"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#varying-alpha-to-get-the-most-accurate-model",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#varying-alpha-to-get-the-most-accurate-model",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Varying alpha to get the most accurate model",
    "text": "Varying alpha to get the most accurate model\n\n\n\n\n\n\n\n\n\n\n\n\nSES alpha\nMAPE\n\nRMSE\n\n\n\n\n\n\n\n0.12\n0.052619\n\n74.00847195\n\n\n\n\n\n0.20925\n0.04772\nLEAST MAPE\n69.79034\n\n\n\n\n\n0.29816\n0.048748836\n\n68.96512784\nLEAST RMSE\n\n\n\n\n0.4\n0.05095862\n\n69.57546188\n\n\n\n\n\n0.9\n0.062207335\n\n81.68324857"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#conclusion",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#conclusion",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Conclusion",
    "text": "Conclusion\nBy varying Alpha we are merely trying to fit the model & minimize the error to historical data. Such tweaking will not necessarily produce a forecast. Also the coefficient of variation needs to be looked at. Higher CV means that data is more volatile & thus Alpha needs to be high to follow these fast changes.\nAlso increasing Alpha does not change the forecast much. This shows the robustness of the SES model.\nFrom the above data, an increase between 0.15 and 0.20 would give a good forecasting model. But whatever the value of Alpha to be used in the model, it needs to be tested on new data to see how it performs."
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "",
    "text": "Objective"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#initial-seasonality-factors",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#initial-seasonality-factors",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Initial Seasonality Factors",
    "text": "Initial Seasonality Factors\nThe above diagram clearly points to a seasonality of the sales.\nSince at this point it is not very clear as to whether there is a trend in the data or not, we find use two methods to find the seasonality factors.\n\nAssuming no trend\nWith no trend the seasonality factors (SF) need not be normalized each season.\nSF per period = total sales per shift / (total sales per month/no. of periods)\nAlso\nSF per period = total sales per shift / average no. of sales per period\nMathematically we can express it as\n\\[\nF_{t}=\\frac{\\sum_{t=1}^{n} D}{(\\sum_{t=1}^{n} D_{t})/P}\n\\]\n\n\nCentered Moving Average Method (CMA)\nSince each season has 4 periods, we use 4‐point Centered Moving Average. Here since the season has an even number of points. We need to take the moving average of the season from both sides & then take the final average.\nBelow is a sample of the data used to calculate part of the Fi’s\nMATop is the average of Shift 1,2,3 & 4\nMABottom is the average of Shift 2,3,4 & 5\nMA_Avgi is the average of MATop & MABottom.\nEach Fi is the xi/MA_Avgi except the first two & last two of the time series. The first two & last two Fi are calculated by first & the last MA_Avg values respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Period d(t)\nDate\nShift Number\nPints Sold, xi\nMATop\nMABottom\nMA_Avg i\nFi\n\n\n\n\n1\n1‐Jun\n1\n357\n\n\n\n\n\n\n2\n1‐Jun\n2\n49\n\n\n\n\n\n\n3\n1‐Jun\n3\n242\n260\n264\n262\n\n\n\n4\n1‐Jun\n4\n391\n264\n264\n264\n\n\n\n5\n2‐Jun\n1\n373\n264\n264\n264\n\n\n\n6\n2‐Jun\n2\n50\n264\n269\n266\n\n\n\n7\n2‐Jun\n3\n243\n269\n269\n269\n\n\n\n8\n2‐Jun\n4\n408\n269\n269\n269\n\n\n\n\nNow if the assumption is incorrect & then is a small trend, then the sum of the factors will not add up to number of periods in a season. i.e P = 4 Hence a correction is required in the form and we simply multiply each of your Seasonality Factors by\n\\[\n\\frac{P}{\\sum_{i=1}^{n} F_{i}}\n\\]\nOnce all the Fi are calculated, we average them according to Shift Number. The summary is in the table.\n\n\n\n\n\n\n\n\n\n\n\nTotal Pints Sold\nIf equal sales per shift, pints per shift sold\nRatio of Sales per shift compared with average\n4‐point Moving Centered Averaged Seasonality Factors\n\n\n\n\nEntire Month\n37423\n\n\n\n\n\nShift 1\n13045\n9356\n1.39432969\n1.402007\n\n\nShift 2\n1737\n9356\n0.185661224\n0.185922\n\n\nShift 3\n8700\n9356\n0.929909414\n0.928191\n\n\nShift 4\n13941\n9356\n1.490099671\n1.483154\n\n\n\n\n\n4\n3.999273"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#holtwinter-model-levelseasonalitytrend",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#holtwinter-model-levelseasonalitytrend",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Holt­Winter Model (level+seasonality+trend)",
    "text": "Holt­Winter Model (level+seasonality+trend)\nLevel & Trend:\nRunning a linear regression we get the equation.\ny = 0.812x + 262.6\nFrom the regression equation we get a level of about 263 pints of beer per shift with an trend of 0.8 additional pints per time period. i.e.\nThe regression gives you an estimated level of 265 pints per each shift with a trend of 0.80 additional pints per time period. This means that the sales of beer is increasing about 3.2 pints per day. Hence there is a positive trend trend.\n\n\nSeasonality\nThis involves estimating the initial values of the level and trend “de‐seasoning” the actual demand by the Seasonality Factors we just found. Part of the data used to calculate the normalized seasonality factors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Period (t)\nDate\nShift Number\nPints Sold\nMATop\nMABottom\nMA_Avg\nFi\nSUM of each season\nFi Normalized\nNormalized Sum\n—\n\n\n\n\n1\n1‐Jun\n1\n357\n\n\n\n1.363897\n3.957406897\n1.378576309\n4\n\n\n\n2\n1‐Jun\n2\n49\n\n\n\n0.187202\n3.957406897\n0.189216356\n\n\n\n\n3\n1‐Jun\n3\n242\n260\n264\n262\n0.924546\n3.957406897\n0.934497106\n\n\n\n\n4\n1‐Jun\n4\n391\n264\n264\n264\n1.481762\n3.957406897\n1.497710229\n\n\n\n\n5\n2‐Jun\n1\n373\n264\n264\n264\n1.41221\n4.023368199\n1.404007844\n4\n\n\n\n6\n2‐Jun\n2\n50\n264\n269\n266\n0.187705\n4.023368199\n0.186615088\n\n\n\n\n7\n2‐Jun\n3\n243\n269\n269\n269\n0.904607\n4.023368199\n0.89935273\n\n\n\n\n8\n2‐Jun\n4\n408\n269\n269\n269\n1.518846\n4.023368199\n1.510024337\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo taking the average of the all the normalized factors we get,\n\n\n\n\nBefore Normalized\nAfter Normalized\n\n\n\n\n\n\nFs1\n1.402007\n1.402205906\n\n\n\n\nFs2\n0.185922\n0.185977102\n\n\n\n\nFs3\n0.928191\n0.928395676\n\n\n\n\nFs4\n1.483154\n1.483421316\n\n\n\n\nSUM\n3.999273\n4\n\n\n\n\n\n\n\nInitial Parameters\nAssume that Alpha=0.15, Beta=0.06 & gamma = 0.05\n\\[\n\\hat{x}_{t,t+\\tau}=(\\hat{a}_{t}+\\tau\\hat{b}_{t})\\hat{F}_{t+\\tau-P}\n\\] \\[\n\\hat{a}_{t}=\\alpha\\left(\\frac{x_{t}}{\\hat{F}_{t-P}}\\right)+(1-\\alpha)(\\hat{a}_{t-1}+\\hat{b}_{t-1})\n\\] \\[\n\\hat{b}_{t}=\\beta(\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta)\\hat{b}_{t-1}\n\\] \\[\n\\hat{F}_{t}=\\gamma \\left(\\frac{x_{t}}{\\hat{a}_{t}}\\right)+(1-\\gamma)\\hat{F}_{t-P}\n\\]\nWe have for the period 120, the following initial parameters,\n\n\n\nFs1\n1.402205906\n\n\n\nFs2\n0.185977102\n\n\n\nFs3\n0.928395676\n\n\n\nFs4\n1.483421316\n\n\n\n\\(\\hat{a}_{120}\\)\n360.04\n0.812*(120) + 262.6\n\n\n\\(\\hat{a}_{120}\\)\n0.812\n\n\n\nAlpha\n0.15\n\n\n\nBeta\n0.06\n\n\n\nGamma\n0.05\n\n\n\n\nUsing above data we can start forecasting for the coming periods 122 i.e. July 2 Shift 2\n\n\n\n\n\n\n\n\n\n\n\n\nActual x(t)\n\\(\\hat{a}_{i}\\)\n\\(\\hat{b}_{i}\\)\n\\(\\hat{F}_{i}\\)\n\\(\\hat{x}_{t+4}\\)\n\n\n\n\n120\n557\n360\n0.81\n\n\n\n\n121\n520\n362.3151378\n0.900308265\n1.403856344\n513.694 (for t=125)\n\n\n\nUsing just the 122 forecast, the rest of the periods i.e. 123, 124 & 125 can be calculated using\n\\[\n\\hat{x}_{t,t+\\tau}=(\\hat{a}_{t}+\\tau\\hat{b}_{t}) \\hat{F}_{t+\\tau-P}\n\\]"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#conclusion",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#conclusion",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Conclusion",
    "text": "Conclusion\nAs observed above, the data about beer consumption follows seasonality & has a positive trend. This can be modeled using the Holt‐Winter Model. Of course, error analysis must be done to tweak the model especially the seasonality factors."
  },
  {
    "objectID": "posts/cs10-extractinginitialparametersexistingholtwinter/index.html",
    "href": "posts/cs10-extractinginitialparametersexistingholtwinter/index.html",
    "title": "Extracting initial parameters from an existing Holt‐Winter forecasting model",
    "section": "",
    "text": "Objective\nThe model is known but the initial parameters need to be found.\n\nIntroduction\nYou are hired by a local company to help them improve their forecasting capabilities. You are tasked with coming up with quarterly forecasts for an item that appears to have level, seasonality, and trend. The good news is that the company has an existing Holt‐Winter forecasting model. The bad news is that no one knows what the parameters (Alpha, Beta, or Gamma) are.\nYou do have some information. For example, you know that historically, the demand in each quarter follows this distribution:\n\nQ1 (January through March) = 50% of average quarterly demand\nQ2 (April through June) = 75% of average quarterly demand\nQ3 (July through September) = 150% of average quarterly demand\nQ4 (October through December) = 125% of average quarterly demand.\n\nYou just ran the forecast at the end of September (end of 2014Q3) and you have the following estimates:\nFor level: \\(\\hat{a}_{2014Q3}\\) = 1052 units\nFor trend: \\(\\hat{b}_{2014Q3}\\) = 46.2 units per quarter\nQ1. What is the forecast for demand for 2014Q4?\nWe know that\n\\(\\hat{x}_{t,t+\\tau}=(\\hat{a_{t}}+\\tau\\hat{b_{t}})\\hat{F}_{t+\\tau-P}\\)\n\\(\\hat{a}_{t}=\\alpha \\left(\\frac{x_{t}}{\\hat{F}_{t-P}}\\right) +(1-\\alpha)(\\hat{a}_{t-1}+\\hat{b}_{t-1})\\)\n\\(\\hat{b}_{t}=\\beta(\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta)\\hat{b}_{t-1}\\)\n\\(\\hat{F}_{t}=\\gamma \\left(\\frac{x_{t}}{\\hat{a}_{t}}\\right) +(1-\\gamma)\\hat{F}_{t-P}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\\(\\hat{a}\\)\n\\(\\hat{b}\\)\n\\(\\hat{F}\\)\n\\(\\hat{x}\\)\n\\(\\hat{F}_{t-P}\\)\n\n\n\n\n2014Q3\n\n1052\n46.2\n\n1372.75\n1.25 (2013Q4)\n\n\n2014Q4\n\n\n\n\n\n\n\n\n\nQ2. Suppose the actual demand in 2014Q4 is 1100 units. What is the smallest & largest possible value for your estimate for level, \\(\\hat{a}_{2014Q4}\\)?\nThe fourth quarter level estimate = \\(\\hat{a}_{2014Q4} = \\hat{x}_{2014Q4} / \\hat{F}_{2013Q4}\\)\nWithout seasonality, level estimate = \\(\\hat{a}_{2014Q4} = (\\hat{a}_{2014Q3}+\\hat{b}_{2014Q34})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha\n\nx\n\n\\(\\hat{a}\\)\n\\(\\hat{b}\\)\n\\(\\hat{F}\\)\n\\(\\hat{x}\\)\n\\(\\hat{a}_{t-P}\\)\n\n\n\n\n\n\n2014Q3\n\n\n1052\n46.2\n\n1372.75\n1.25 (2013Q4)\n\n\nSmallest\n1\n2014Q4\n\n1100\n880\n\n\n\n\n\n\nLargest\n0\n2014Q4\n\n1100\n1098.2\n\n\n\n\n\n\n\nQ3. The model was run at the end of 2014Q4. It provided you with the most recent estimates of each pattern. A) The estimate for level, \\(\\hat{a}_{2014Q4}\\) was 1065.5. What is value of alpha? B) Estimate of trend, \\(\\hat{b}_{2014Q4}\\) = 42.9, what is value of beta? C) Estimate of seasonality is \\(\\hat{F}_{2014Q4}\\) = 1.239\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nGiven\nEquation\n\n\nParameter\nSolved\n\n\n\n\n\\(\\hat{a}_{2014Q4}\\)\n1065.5\n1065.5\\(=\\alpha \\left(\\frac{1100}{1.25}\\right) +(1-\\alpha)(1052+46.2)\\)\n1100\n1052 46.2\nAlpha\n0.15\n\n\n\\(\\hat{b}_{2014Q4}\\)\n42.9\n42.9\\(=\\beta(1.065.5-1052)+(1-\\beta)46.2\\)\n1065.5 1052\n1 46.2\nBeta\n0.1\n\n\n\\(\\hat{F}_{2014Q4}\\)\n1.239\n1.239\\(=\\gamma \\left(\\frac{1100}{1065.5}\\right) +(1-\\gamma)1.25\\)\n1100\n1 1.25\nGamma\n0.05\n\n\n\nQ4. What is your forecast for demand for the 1st quarter of 2015? That is \\(\\hat{a}_{2014Q4,2015Q1}\\)?\n\\(\\hat{x}_{2014Q4,2015Q1} =(\\hat{a}_{2014Q1} + \\hat{b}_{2014Q1}) \\hat{F}_{2014Q4}\\)\nWe have the unnormalized seasonality factor, \\(\\hat{F}_{2014Q4} = 1.239\\)\nSince the sum of the most recent season estimates (0.500, 0.750, 1.500, and 1.239 for Q1, Q2, Q3, and\nQ4) adds up to 3.98912, , we need to normalize \\(\\hat{F}_{2014Q4}\\) before we use it in our calculations. We use the formula\n\\[\n\\hat{F}_{iadj}=\\hat{F}_{iold}\\frac{P}{\\sum{\\hat{F}_{i}}}\n\\]\nSo we have \\(\\hat{F}_{2014Q1} = 0.500*(4.000/3.989) = 0.50136\\)\n\\(\\hat{x}_{2014Q4,2015Q1} =(\\hat{a}_{2014Q1} + \\hat{b}_{2014Q1}) \\hat{F}_{2014Q1}\\)\n\\(\\hat{x}_{2014Q4,2015Q1} = (1065.5 + 42.9)(0.501) = 555.3084 = 555.31\\)\nIf you did not normalize the seasonality factor you would have gotten = (1065.5 + 42.9)(0.500) = 554.20. Normalizing the seasonality factors prevents the estimates from drifting. In this case, it is a small drift ‐ but over time it would grow."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/sql/index.html",
    "href": "posts/sql/index.html",
    "title": "SQL Guide.",
    "section": "",
    "text": "This is a long guide to mastering SQL."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "sql-mode.html",
    "href": "sql-mode.html",
    "title": "INNER JOIN",
    "section": "",
    "text": "This uses PGSQL for syntax\n– Show version information of the database select Version()\nversion PostgreSQL 13.7 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-6), 64-bit\n– Show all tables in a particular Schema SELECT * FROM pg_catalog.pg_tables WHERE schemaname='tutorial'\nschemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity tutorial animial_crossing_accessories_9067b7cc3b49da87072a3ca5 mode_admin false false false false tutorial animal_crossing_achievements_28e4e04e975e4f27cb9be9e1 mode_admin false false false false tutorial animal_crossing_art_3a053143a5a8c3ae9b75d0ed mode_admin false false false false tutorial animal_crossing_bags_9de2101d615e87fd86a3e478 mode_admin false false false false tutorial animal_crossing_bottoms_7b62ddba747aef4ef8c8d8a2 mode_admin false false false false\n– Describe table details\nSELECT * FROM information_schema.columns\nWHERE table_schema = 'tutorial' AND table_name   = 'us_housing_units'\ntable_catalog table_schema table_name column_name ordinal_position column_default is_nullable data_type character_maximum_length character_octet_length numeric_precision numeric_precision_radix numeric_scale datetime_precision interval_type interval_precision character_set_catalog character_set_schema character_set_name collation_catalog collation_schema collation_name domain_catalog domain_schema domain_name udt_catalog udt_schema udt_name scope_catalog scope_schema scope_name maximum_cardinality dtd_identifier is_self_referencing is_identity identity_generation identity_start identity_increment identity_maximum identity_minimum identity_cycle is_generated generation_expression is_updatable d5b78k6rg4etlv tutorial us_housing_units year 1 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 1 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units month 2 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 2 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units month_name 3 YES text 1073741824 d5b78k6rg4etlv pg_catalog text 3 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units south 4 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 4 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units west 5 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 5 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units midwest 6 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 6 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units northeast 7 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 7 NO NO NO NEVER YES SQL Tutorial\nBasic SQL\n    The SQL Tutorial for Data Analysis\n    Using SQL in Mode\n    SQL SELECT\n    SQL LIMIT\n    SQL WHERE\n    SQL Comparison Operators\n    SQL Logical Operators\n    SQL LIKE\n    SQL IN\n    SQL BETWEEN\n    SQL IS NULL\n    SQL AND\n    SQL OR\n    SQL NOT\n    SQL ORDER BY\n– Note: the clauses always need to be in this order: SELECT, FROM, WHERE.\n– Comparison operators on numerical data: =, &lt;&gt; or !=, &gt;, &lt;, &gt;=, &lt;=\n– Comparison operators on non-numerical data: Same as above. = and != make perfect sense—they allow you to select rows that match or don’t match any string value, respectively. Query in which none of the January rows show up:\nSELECT * FROM tutorial.us_housing_units WHERE month_name != ‘January’\n– If you’re using an operator with values that are non-numeric, you need to put the value in single quotes: ‘value’.\nNote: SQL uses single quotes to reference column values.\n– ?? You can use &gt;, &lt;, and the rest of the comparison operators on non-numeric columns as well—they filter based on alphabetical order.\n– In the above query that selecting month_name &gt; ‘J’ will yield only rows in which month_name starts with “j” or later in the alphabet. “Wait a minute,” you might say. “January is included in the results—shouldn’t I have to use month_name &gt;= ‘J’ to make that happen?” SQL considers ‘Ja’ to be greater than ‘J’ because it has an extra letter. It’s worth noting that most dictionaries would list ‘Ja’ after ‘J’ as well.\nSELECT * FROM tutorial.us_housing_units WHERE month_name &gt; ‘January’\nIf you’re using &gt;, &lt;, &gt;=, or &lt;=, you don’t necessarily need to be too specific about how you filter. Try this:\nSELECT * FROM tutorial.us_housing_units WHERE month_name &gt; ‘J’\nSELECT * FROM tutorial.us_housing_units WHERE month_name = ‘February’\n– Write a query that only shows rows for which the month_name starts with the letter “N” or an earlier letter in the alphabet.\nSELECT * FROM tutorial.us_housing_units WHERE month_name &lt; ‘o’\n– Below won’t work as it excluded November as n is before November.\nSELECT * FROM tutorial.us_housing_units WHERE month_name &lt;= ‘n’\n– Arithmetic in SQL\nUnlike Excel, in SQL you can only perform arithmetic across columns on values in a given row. To clarify, you can only add values in multiple columns from the same row together using +—. If you want to add values across multiple rows, you’ll need to use aggregate functions.\nSELECT year, month, west, south, west + south - 4 * year AS nonsense_column FROM tutorial.us_housing_units\nThe columns that contain the arithmetic functions are called “derived columns” because they are generated by modifying the information that exists in the underlying data.\n– Write a query that calculates the percentage of all houses completed in the United States represented by each region. Only return results from the year 2000 and later.\nHint: There should be four columns of percentages.\nSELECT year, month, west/(west + south + midwest + northeast)100 AS west_pct, south/(west + south + midwest + northeast)100 AS south_pct, midwest/(west + south + midwest + northeast)100 AS midwest_pct, northeast/(west + south + midwest + northeast)100 AS northeast_pct FROM tutorial.us_housing_units WHERE year &gt;= 2000\n– Logical operators allow you to use multiple comparison operators in one query: LIKE, IN, BETWEEN, IS NULL, AND, OR, NOT\n=======================================\nThe SQL LIKE operator\nLIKE is a logical operator in SQL that allows you to match on similar values rather than exact ones.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” LIKE ‘Snoop%’\nNote: “group” appears in quotations above because GROUP is actually the name of a function in SQL. The double quotes (as opposed to single: ’) are a way of indicating that you are referring to the column name “group”, not the SQL function. In general, putting double quotes around a word or phrase will indicate that you are referring to that column name.\nWildcards and ILIKE\nThe % used above represents any character or set of characters. In the type of SQL that Mode uses, LIKE is case-sensitive & ILIKE is case-insensitive:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” ILIKE ‘snoop%’\nYou can also use _ (a single underscore) to substitute for an individual character:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist ILIKE ‘dr_ke’\n– Write a query that returns all rows for which Ludacris was a member of the group.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE group_name LIKE ‘Ludacris’\n=========================== The SQL IN operator\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank IN (1, 2, 3)\nAs with comparison operators, you can use non-numerical values, but they need to go inside single quotes. Regardless of the data type, the values in the list must be separated by commas.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist IN (‘Taylor Swift’, ‘Usher’, ‘Ludacris’)\n==================================\n– Write a query that shows all of the entries for Elvis and M.C. Hammer.\nHint: M.C. Hammer is actually on the list under multiple names, so you may need to first write a query to figure out exactly how M.C. Hammer is listed. You’re likely to face similar problems that require some exploration in many real-life scenarios.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist like ‘%Hammer%’ or artist like ‘%Elvis%’\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” IN (‘M.C. Hammer’, ‘Hammer’, ‘Elvis Presley’)\n======================================= The SQL BETWEEN operator\nBETWEEN is a logical operator in SQL that allows you to select only rows that are within a specific range. It has to be paired with the AND operator. BETWEEN includes the range bounds (in this case, 5 and 10)\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank BETWEEN 5 AND 10\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &gt;= 5 AND year_rank &lt;= 10\nSome people prefer the latter example because it more explicitly shows what the query is doing (it’s easy to forget whether or not BETWEEN includes the range bounds).\n– Write a query that shows all top 100 songs from January 1, 1985 through December 31, 1990.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year &gt;= 1985 AND year &lt;= 1990\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year BETWEEN 1985 AND 1990\n=================================\nSQL IS NULL You can select rows that contain no data in a given column by using IS NULL.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist IS NULL\n– WHERE artist = NULL will not work—you can’t perform arithmetic on NULL values.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE song_name is NULL\n– Write a query that surfaces the top-ranked records in 1990, 2000, and 2010.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank = 1 AND year IN (1990, 2000, 2010)\n============================================= – SQL OR You will notice that the conditional statement year = 2013 will be fulfilled for every row returned. In this case, OR must be satisfied in addition to the first statement of year = 2013.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND (“group” ILIKE ‘%macklemore%’ OR “group” ILIKE ‘%timberlake%’)\n– Write a query that returns all rows for top-10 songs that featured either Katy Perry or Bon Jovi.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 10 AND (“group” ILIKE ‘%katy perry%’ OR “group” ILIKE ‘%bon jovi%’)\n– Group??? instead of artist???\n– Write a query that returns all songs with titles that contain the word “California” in either the 1970s or 1990s.\nSELECT * FROM tutorial.billboard_top_100_year_end where title ilike ‘%California%’ and ((year between 1970 and 1979) or (year between 1990 and 1999))\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE song_name LIKE ‘%California%’ AND (year BETWEEN 1970 AND 1979 OR year BETWEEN 1990 AND 1999)\n– brackets priority???? Yes, here the BETWEEN makes sure\n– Write a query that lists all top-100 recordings that feature Dr. Dre before 2001 or after 2009.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” ILIKE ‘%dr. dre%’ AND (year &lt;= 2000 OR year &gt;= 2010)\nSELECT * FROM tutorial.billboard_top_100_year_end where artist ilike ‘%Dr. Dre%’ and (year &lt;2001 or year &gt; 2009)\n– Dates attention to wording????? Possible confusion here.\n===================== – The SQL NOT operator\nNOT is a logical operator in SQL that you can put before any conditional statement to select rows for which that statement is false.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND year_rank NOT BETWEEN 2 AND 3\nIn the above case, you can see that results for which year_rank is equal to 2 or 3 are not included\n\n\n\nUsing NOT with &lt; and &gt; usually doesn’t make sense because you can simply use the opposite comparative operator instead. For example, this query will return an error:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND year_rank NOT &gt; 3\nInstead, you would just write that as:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND year_rank &lt;= 3\nNOT is commonly used with LIKE. Run this query and check out how Macklemore magically disappears!\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND “group” NOT ILIKE ‘%macklemore%’\nNOT is also frequently used to identify non-null rows, but the syntax is somewhat special—you need to include IS beforehand. Here’s how that looks:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND artist IS NOT NULL\n==================================================== Sorting data with SQL ORDER BY\nYou’ll notice that the results are now ordered alphabetically from a to z based on the content in the artist column. This is referred to as ascending order, and it’s SQL’s default. If you order a numerical column in ascending order, it will start with smaller (or most negative) numbers, with each successive row having a higher numerical value than the previous. Here’s an example using a numerical column:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2012 ORDER BY song_name DESC\n=================================== Ordering data by multiple columns. This example query makes the most recent years come first but orders top-ranks songs before lower-ranked songs:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 3 ORDER BY year DESC, year_rank\nYou can see a couple things from the above query: First, columns in the ORDER BY clause must be separated by commas. Second, the DESC operator is only applied to the column that precedes it. Finally, the results are sorted by the first column mentioned (year), then by year_rank afterward. You can see the difference the order makes by running the following query:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 3 ORDER BY year_rank, year DESC\nFinally, you can make your life a little easier by substituting numbers for column names in the ORDER BY clause. The numbers will correspond to the order in which you list columns in the SELECT clause. For example, the following query is exactly equivalent to the previous query:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 3 ORDER BY 2, 1 DESC\nNote: this functionality (numbering columns instead of using names) is supported by ModedotCom, but not by every flavor of SQL, so if you’re using another system or connected to certain types of databases, it may not work.\nWhen using ORDER BY with a row limit (either through the check box on the query editor or by typing in LIMIT), the ordering clause is executed first. This means that the results are ordered before limiting to only a few rows, so if you were to order by year_rank, for example, you can be sure that you are getting the lowest values of year_rank in the entire table, not just in the first 100 rows of the table. i.e. ORDER BY executes first before LIMIT.\n@sqlchallenge1 / accounts\n@sqlchallenge1 / orders\n@sqlchallenge1 / region\n@sqlchallenge1 / sales_reps\n\n\n@sqlchallenge1 / accounts\n\nName    Data Type\nid Number name String website String lat Number long Number primary_poc String sales_rep_id Number\n@sqlchallenge1 / orders\nName    Data Type\nid Number account_id Number occurred_at Date/Time standard_qty Number gloss_qty Number poster_qty Number total Number standard_amt_usd Number gloss_amt_usd Number poster_amt_usd Number total_amt_usd Number\n@sqlchallenge1 / region\nName    Data Type\nid Number name String\n@sqlchallenge1 / sales_reps\n\nName    Data Type\nid Number name String region_id Number\nBeginner level\nQ1. WHich company website has the longest url?\nSELECT website, LENgth(website) as lenw FROM sqlchallenge1.accounts order by lenw desc LIMIT 4\nSELECT website, LENgth(website) FROM sqlchallenge1.accounts order by length(website) desc LIMIT 4\nwebsite length www.unitedcontinentalholdings.com 33\nQ2. How many sales reps have letter ‘e’ in their names?\nselect count(name) from sqlchallenge1.sales_reps where name like ‘%e%’\n39\nQ3. What is the alphabetically first account name that contains an ampersand (“&”)?\nSELECT name FROM sqlchallenge1.accounts where name like ‘%&%’ order by name LIMIT 4\nAir Products & Chemicals\nQ4. What is the id of the Sales Rep that sold the last order in May 2016?\nThe dataset was incorrect and hence had to reverse-engineer the answer.\nselect id,account_id,occurred_at from sqlchallenge1.orders where to_char(occurred_at,‘yyyy-mm-dd’) like ‘2016-05-31’ order by occurred_at desc limit 10\n– select id,sales_rep_id from sqlchallenge1.accounts where id=1441\n– didn’t work as occurred_at is datetime and not a string. – select account_id, occurred_at from sqlchallenge1.orders – where occurred_at like ‘2015-05-31%’ – limit 2\n– not proper as we need occurred_at time also to determine the last order. – select id,account_id,to_char(occurred_at,‘yyyy-mm-dd’) as dd – from sqlchallenge1.orders – where to_char(occurred_at,‘yyyy-mm-dd’) like ‘2016-05-31’ – limit 2\n– reverse engineering from existing answers – select id,sales_rep_id from sqlchallenge1.accounts – where sales_rep_id in (321740,321510,321760,321520) – order by sales_rep_id\n– this gives that 1441 as the proper answer according to the answers but actually incorrect at 3rd place according to orders dataset. – select id,account_id,occurred_at – from sqlchallenge1.orders – where account_id in (1011, 1021, 1221, 1231, 1431, 1441, 1641, 1651, 1671, 1681, 1701, 1711, 1971, 2091, 2111, 2181, 2201, 2271, 2291, 2331, 2351, 2391, 2411, 2451, 2471, 2481, 2501, 2511) – and to_char(occurred_at,‘yyyy-mm’) like ‘2016-05’ – order by occurred_at desc\nQ4. How many sales reps represent the Northeast Region?\nselect region_id,count(id) from sqlchallenge1.sales_reps group by region_id;\nselect * from sqlchallenge1.region\nregion_id count 3 10 4 10 1 21 2 9\nid name 1 Northeast 2 Midwest 3 Southeast 4 West\n============================================================\nSQL Aggregate Functions:\nHere’s a quick preview:\nCOUNT counts how many rows are in a particular column. SUM adds together all the values in a particular column. MIN and MAX return the lowest and highest values in a particular column, respectively. AVG calculates the average of a group of selected values.\nIntermediate SQL Putting it together SQL Aggregate Functions SQL COUNT SQL SUM SQL MIN/MAX SQL AVG SQL GROUP BY SQL HAVING SQL CASE SQL DISTINCT SQL Joins SQL INNER JOIN SQL Outer Joins SQL LEFT JOIN SQL RIGHT JOIN SQL Joins Using WHERE or ON SQL FULL OUTER JOIN SQL UNION SQL Joins with Comparison Operators SQL Joins on Multiple Keys SQL Self Joins\nSELECT COUNT(*) FROM tutorial.aapl_historical_stock_price\nNote: Typing COUNT(1) has the same effect as COUNT(*). Which one you use is a matter of personal preference.\nThe following code will provide a count of all of rows in which the high column is not null. COUNT cannot count NULL i.e. will not include NULL rows in the count.\nSELECT COUNT(high) FROM tutorial.aapl_historical_stock_price\nYou’ll notice that this result is lower than what you got with COUNT(*). That’s because high has some NULLs. In this case, we’ve deleted some data to make the lesson interesting, but analysts often run into naturally-occurring null rows.\n===========================\nSELECT COUNT(date) AS count_of_date FROM tutorial.aapl_historical_stock_price\nIf you must use spaces, you will need to use double quotes.\nSELECT COUNT(date) AS “Count Of Date” FROM tutorial.aapl_historical_stock_price\nNote: This is really the only place in which you’ll ever want to use double quotes in SQL. Single quotes for everything else.\n– In pgsql, double quotes for column names and derived column names. Everything else can be single quotes including strings.\n– Write a query that determines counts of every single column. With these counts, can you tell which column has the most null values?\nSELECT COUNT(date) as date, count(year) as year,count(month) as month, count(open) as open,count(high) as high, count(low) as low,count(close) as close, count(volume) as volume, count(id) as id FROM tutorial.aapl_historical_stock_price limit 1\n=========================================================================\nThe SQL SUM function\nSUM is a SQL aggregate function. that totals the values in a given column. Unlike COUNT, you can only use SUM on columns containing numerical values.\nThe query below selects the sum of the volume column from the Apple stock prices dataset:\nSELECT SUM(volume) FROM tutorial.aapl_historical_stock_price\nAn important thing to remember: aggregators only aggregate vertically. If you want to perform a calculation across rows, you would do this with simple arithmetic.\nYou don’t need to worry as much about the presence of nulls with SUM as you would with COUNT, as SUM treats nulls as 0. i.e. COUNT skips NULLs but SUM treats them as 0.\n– Write a query to calculate the average opening price (hint: you will need to use both COUNT and SUM, as well as some simple arithmetic.).\nSELECT sum(open)/count(open),sum(open)/count(1) FROM tutorial.aapl_historical_stock_price limit 1\n– This says that sum(open)/count(open) is right but there are events where there is no opening price? so should not sum(open)/count(1) be proper?\nThe SQL MIN and MAX functions\nMIN and MAX are SQL aggregation functions that return the lowest and highest values in a particular column.\nThey’re similar to COUNT in that they can be used on non-numerical columns. Depending on the column type, MIN will return the lowest number, earliest date, or non-numerical value as close alphabetically to “A” as possible. As you might suspect, MAX does the opposite—it returns the highest number, the latest date, or the non-numerical value closest alphabetically to “Z.”\n– &gt; COUNT, MIN, MAX can be used in non-numerical columns.\nThe SQL AVG function\nAVG is a SQL aggregate function that calculates the average of a selected group of values. It’s very useful, but has some limitations. First, it can only be used on numerical columns. Second, it ignores nulls completely. You can see this by comparing these two queries of the Apple stock prices dataset:\nSELECT AVG(high) FROM tutorial.aapl_historical_stock_price WHERE high IS NOT NULL\nThe above query produces the same result as the following query:\nSELECT AVG(high) FROM tutorial.aapl_historical_stock_price\nThere are some cases in which you’ll want to treat null values as 0. For these cases, you’ll want to write a statement that changes the nulls to 0 (covered in a later lesson.\n??? Make examples of all Aggregate functions with and without NULL condition and check.\nThe SQL GROUP BY clause\nSQL aggregate function like COUNT, AVG, and SUM have something in common: they all aggregate across the entire table. But what if you want to aggregate only part of a table? For example, you might want to count the number of entries for each year.\nIn situations like this, you’d need to use the GROUP BY clause. GROUP BY allows you to separate data into groups, which can be aggregated independently of one another. Here’s an example using the Apple stock prices dataset:\nSELECT year, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY year\n(You can group by multiple columns, but you have to separate column names with commas—just as with ORDER BY):\nSELECT year, month, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY year, month"
  },
  {
    "objectID": "sql-mode.html#mode.com-sql-notes",
    "href": "sql-mode.html#mode.com-sql-notes",
    "title": "INNER JOIN",
    "section": "",
    "text": "This uses PGSQL for syntax\n– Show version information of the database select Version()\nversion PostgreSQL 13.7 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 7.3.1 20180712 (Red Hat 7.3.1-6), 64-bit\n– Show all tables in a particular Schema SELECT * FROM pg_catalog.pg_tables WHERE schemaname='tutorial'\nschemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity tutorial animial_crossing_accessories_9067b7cc3b49da87072a3ca5 mode_admin false false false false tutorial animal_crossing_achievements_28e4e04e975e4f27cb9be9e1 mode_admin false false false false tutorial animal_crossing_art_3a053143a5a8c3ae9b75d0ed mode_admin false false false false tutorial animal_crossing_bags_9de2101d615e87fd86a3e478 mode_admin false false false false tutorial animal_crossing_bottoms_7b62ddba747aef4ef8c8d8a2 mode_admin false false false false\n– Describe table details\nSELECT * FROM information_schema.columns\nWHERE table_schema = 'tutorial' AND table_name   = 'us_housing_units'\ntable_catalog table_schema table_name column_name ordinal_position column_default is_nullable data_type character_maximum_length character_octet_length numeric_precision numeric_precision_radix numeric_scale datetime_precision interval_type interval_precision character_set_catalog character_set_schema character_set_name collation_catalog collation_schema collation_name domain_catalog domain_schema domain_name udt_catalog udt_schema udt_name scope_catalog scope_schema scope_name maximum_cardinality dtd_identifier is_self_referencing is_identity identity_generation identity_start identity_increment identity_maximum identity_minimum identity_cycle is_generated generation_expression is_updatable d5b78k6rg4etlv tutorial us_housing_units year 1 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 1 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units month 2 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 2 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units month_name 3 YES text 1073741824 d5b78k6rg4etlv pg_catalog text 3 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units south 4 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 4 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units west 5 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 5 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units midwest 6 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 6 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units northeast 7 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 7 NO NO NO NEVER YES SQL Tutorial\nBasic SQL\n    The SQL Tutorial for Data Analysis\n    Using SQL in Mode\n    SQL SELECT\n    SQL LIMIT\n    SQL WHERE\n    SQL Comparison Operators\n    SQL Logical Operators\n    SQL LIKE\n    SQL IN\n    SQL BETWEEN\n    SQL IS NULL\n    SQL AND\n    SQL OR\n    SQL NOT\n    SQL ORDER BY\n– Note: the clauses always need to be in this order: SELECT, FROM, WHERE.\n– Comparison operators on numerical data: =, &lt;&gt; or !=, &gt;, &lt;, &gt;=, &lt;=\n– Comparison operators on non-numerical data: Same as above. = and != make perfect sense—they allow you to select rows that match or don’t match any string value, respectively. Query in which none of the January rows show up:\nSELECT * FROM tutorial.us_housing_units WHERE month_name != ‘January’\n– If you’re using an operator with values that are non-numeric, you need to put the value in single quotes: ‘value’.\nNote: SQL uses single quotes to reference column values.\n– ?? You can use &gt;, &lt;, and the rest of the comparison operators on non-numeric columns as well—they filter based on alphabetical order.\n– In the above query that selecting month_name &gt; ‘J’ will yield only rows in which month_name starts with “j” or later in the alphabet. “Wait a minute,” you might say. “January is included in the results—shouldn’t I have to use month_name &gt;= ‘J’ to make that happen?” SQL considers ‘Ja’ to be greater than ‘J’ because it has an extra letter. It’s worth noting that most dictionaries would list ‘Ja’ after ‘J’ as well.\nSELECT * FROM tutorial.us_housing_units WHERE month_name &gt; ‘January’\nIf you’re using &gt;, &lt;, &gt;=, or &lt;=, you don’t necessarily need to be too specific about how you filter. Try this:\nSELECT * FROM tutorial.us_housing_units WHERE month_name &gt; ‘J’\nSELECT * FROM tutorial.us_housing_units WHERE month_name = ‘February’\n– Write a query that only shows rows for which the month_name starts with the letter “N” or an earlier letter in the alphabet.\nSELECT * FROM tutorial.us_housing_units WHERE month_name &lt; ‘o’\n– Below won’t work as it excluded November as n is before November.\nSELECT * FROM tutorial.us_housing_units WHERE month_name &lt;= ‘n’\n– Arithmetic in SQL\nUnlike Excel, in SQL you can only perform arithmetic across columns on values in a given row. To clarify, you can only add values in multiple columns from the same row together using +—. If you want to add values across multiple rows, you’ll need to use aggregate functions.\nSELECT year, month, west, south, west + south - 4 * year AS nonsense_column FROM tutorial.us_housing_units\nThe columns that contain the arithmetic functions are called “derived columns” because they are generated by modifying the information that exists in the underlying data.\n– Write a query that calculates the percentage of all houses completed in the United States represented by each region. Only return results from the year 2000 and later.\nHint: There should be four columns of percentages.\nSELECT year, month, west/(west + south + midwest + northeast)100 AS west_pct, south/(west + south + midwest + northeast)100 AS south_pct, midwest/(west + south + midwest + northeast)100 AS midwest_pct, northeast/(west + south + midwest + northeast)100 AS northeast_pct FROM tutorial.us_housing_units WHERE year &gt;= 2000\n– Logical operators allow you to use multiple comparison operators in one query: LIKE, IN, BETWEEN, IS NULL, AND, OR, NOT\n=======================================\nThe SQL LIKE operator\nLIKE is a logical operator in SQL that allows you to match on similar values rather than exact ones.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” LIKE ‘Snoop%’\nNote: “group” appears in quotations above because GROUP is actually the name of a function in SQL. The double quotes (as opposed to single: ’) are a way of indicating that you are referring to the column name “group”, not the SQL function. In general, putting double quotes around a word or phrase will indicate that you are referring to that column name.\nWildcards and ILIKE\nThe % used above represents any character or set of characters. In the type of SQL that Mode uses, LIKE is case-sensitive & ILIKE is case-insensitive:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” ILIKE ‘snoop%’\nYou can also use _ (a single underscore) to substitute for an individual character:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist ILIKE ‘dr_ke’\n– Write a query that returns all rows for which Ludacris was a member of the group.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE group_name LIKE ‘Ludacris’\n=========================== The SQL IN operator\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank IN (1, 2, 3)\nAs with comparison operators, you can use non-numerical values, but they need to go inside single quotes. Regardless of the data type, the values in the list must be separated by commas.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist IN (‘Taylor Swift’, ‘Usher’, ‘Ludacris’)\n==================================\n– Write a query that shows all of the entries for Elvis and M.C. Hammer.\nHint: M.C. Hammer is actually on the list under multiple names, so you may need to first write a query to figure out exactly how M.C. Hammer is listed. You’re likely to face similar problems that require some exploration in many real-life scenarios.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist like ‘%Hammer%’ or artist like ‘%Elvis%’\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” IN (‘M.C. Hammer’, ‘Hammer’, ‘Elvis Presley’)\n======================================= The SQL BETWEEN operator\nBETWEEN is a logical operator in SQL that allows you to select only rows that are within a specific range. It has to be paired with the AND operator. BETWEEN includes the range bounds (in this case, 5 and 10)\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank BETWEEN 5 AND 10\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &gt;= 5 AND year_rank &lt;= 10\nSome people prefer the latter example because it more explicitly shows what the query is doing (it’s easy to forget whether or not BETWEEN includes the range bounds).\n– Write a query that shows all top 100 songs from January 1, 1985 through December 31, 1990.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year &gt;= 1985 AND year &lt;= 1990\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year BETWEEN 1985 AND 1990\n=================================\nSQL IS NULL You can select rows that contain no data in a given column by using IS NULL.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE artist IS NULL\n– WHERE artist = NULL will not work—you can’t perform arithmetic on NULL values.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE song_name is NULL\n– Write a query that surfaces the top-ranked records in 1990, 2000, and 2010.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank = 1 AND year IN (1990, 2000, 2010)\n============================================= – SQL OR You will notice that the conditional statement year = 2013 will be fulfilled for every row returned. In this case, OR must be satisfied in addition to the first statement of year = 2013.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND (“group” ILIKE ‘%macklemore%’ OR “group” ILIKE ‘%timberlake%’)\n– Write a query that returns all rows for top-10 songs that featured either Katy Perry or Bon Jovi.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 10 AND (“group” ILIKE ‘%katy perry%’ OR “group” ILIKE ‘%bon jovi%’)\n– Group??? instead of artist???\n– Write a query that returns all songs with titles that contain the word “California” in either the 1970s or 1990s.\nSELECT * FROM tutorial.billboard_top_100_year_end where title ilike ‘%California%’ and ((year between 1970 and 1979) or (year between 1990 and 1999))\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE song_name LIKE ‘%California%’ AND (year BETWEEN 1970 AND 1979 OR year BETWEEN 1990 AND 1999)\n– brackets priority???? Yes, here the BETWEEN makes sure\n– Write a query that lists all top-100 recordings that feature Dr. Dre before 2001 or after 2009.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE “group” ILIKE ‘%dr. dre%’ AND (year &lt;= 2000 OR year &gt;= 2010)\nSELECT * FROM tutorial.billboard_top_100_year_end where artist ilike ‘%Dr. Dre%’ and (year &lt;2001 or year &gt; 2009)\n– Dates attention to wording????? Possible confusion here.\n===================== – The SQL NOT operator\nNOT is a logical operator in SQL that you can put before any conditional statement to select rows for which that statement is false.\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND year_rank NOT BETWEEN 2 AND 3\nIn the above case, you can see that results for which year_rank is equal to 2 or 3 are not included\n\n\n\nUsing NOT with &lt; and &gt; usually doesn’t make sense because you can simply use the opposite comparative operator instead. For example, this query will return an error:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND year_rank NOT &gt; 3\nInstead, you would just write that as:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND year_rank &lt;= 3\nNOT is commonly used with LIKE. Run this query and check out how Macklemore magically disappears!\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND “group” NOT ILIKE ‘%macklemore%’\nNOT is also frequently used to identify non-null rows, but the syntax is somewhat special—you need to include IS beforehand. Here’s how that looks:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2013 AND artist IS NOT NULL\n==================================================== Sorting data with SQL ORDER BY\nYou’ll notice that the results are now ordered alphabetically from a to z based on the content in the artist column. This is referred to as ascending order, and it’s SQL’s default. If you order a numerical column in ascending order, it will start with smaller (or most negative) numbers, with each successive row having a higher numerical value than the previous. Here’s an example using a numerical column:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year = 2012 ORDER BY song_name DESC\n=================================== Ordering data by multiple columns. This example query makes the most recent years come first but orders top-ranks songs before lower-ranked songs:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 3 ORDER BY year DESC, year_rank\nYou can see a couple things from the above query: First, columns in the ORDER BY clause must be separated by commas. Second, the DESC operator is only applied to the column that precedes it. Finally, the results are sorted by the first column mentioned (year), then by year_rank afterward. You can see the difference the order makes by running the following query:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 3 ORDER BY year_rank, year DESC\nFinally, you can make your life a little easier by substituting numbers for column names in the ORDER BY clause. The numbers will correspond to the order in which you list columns in the SELECT clause. For example, the following query is exactly equivalent to the previous query:\nSELECT * FROM tutorial.billboard_top_100_year_end WHERE year_rank &lt;= 3 ORDER BY 2, 1 DESC\nNote: this functionality (numbering columns instead of using names) is supported by ModedotCom, but not by every flavor of SQL, so if you’re using another system or connected to certain types of databases, it may not work.\nWhen using ORDER BY with a row limit (either through the check box on the query editor or by typing in LIMIT), the ordering clause is executed first. This means that the results are ordered before limiting to only a few rows, so if you were to order by year_rank, for example, you can be sure that you are getting the lowest values of year_rank in the entire table, not just in the first 100 rows of the table. i.e. ORDER BY executes first before LIMIT.\n@sqlchallenge1 / accounts\n@sqlchallenge1 / orders\n@sqlchallenge1 / region\n@sqlchallenge1 / sales_reps\n\n\n@sqlchallenge1 / accounts\n\nName    Data Type\nid Number name String website String lat Number long Number primary_poc String sales_rep_id Number\n@sqlchallenge1 / orders\nName    Data Type\nid Number account_id Number occurred_at Date/Time standard_qty Number gloss_qty Number poster_qty Number total Number standard_amt_usd Number gloss_amt_usd Number poster_amt_usd Number total_amt_usd Number\n@sqlchallenge1 / region\nName    Data Type\nid Number name String\n@sqlchallenge1 / sales_reps\n\nName    Data Type\nid Number name String region_id Number\nBeginner level\nQ1. WHich company website has the longest url?\nSELECT website, LENgth(website) as lenw FROM sqlchallenge1.accounts order by lenw desc LIMIT 4\nSELECT website, LENgth(website) FROM sqlchallenge1.accounts order by length(website) desc LIMIT 4\nwebsite length www.unitedcontinentalholdings.com 33\nQ2. How many sales reps have letter ‘e’ in their names?\nselect count(name) from sqlchallenge1.sales_reps where name like ‘%e%’\n39\nQ3. What is the alphabetically first account name that contains an ampersand (“&”)?\nSELECT name FROM sqlchallenge1.accounts where name like ‘%&%’ order by name LIMIT 4\nAir Products & Chemicals\nQ4. What is the id of the Sales Rep that sold the last order in May 2016?\nThe dataset was incorrect and hence had to reverse-engineer the answer.\nselect id,account_id,occurred_at from sqlchallenge1.orders where to_char(occurred_at,‘yyyy-mm-dd’) like ‘2016-05-31’ order by occurred_at desc limit 10\n– select id,sales_rep_id from sqlchallenge1.accounts where id=1441\n– didn’t work as occurred_at is datetime and not a string. – select account_id, occurred_at from sqlchallenge1.orders – where occurred_at like ‘2015-05-31%’ – limit 2\n– not proper as we need occurred_at time also to determine the last order. – select id,account_id,to_char(occurred_at,‘yyyy-mm-dd’) as dd – from sqlchallenge1.orders – where to_char(occurred_at,‘yyyy-mm-dd’) like ‘2016-05-31’ – limit 2\n– reverse engineering from existing answers – select id,sales_rep_id from sqlchallenge1.accounts – where sales_rep_id in (321740,321510,321760,321520) – order by sales_rep_id\n– this gives that 1441 as the proper answer according to the answers but actually incorrect at 3rd place according to orders dataset. – select id,account_id,occurred_at – from sqlchallenge1.orders – where account_id in (1011, 1021, 1221, 1231, 1431, 1441, 1641, 1651, 1671, 1681, 1701, 1711, 1971, 2091, 2111, 2181, 2201, 2271, 2291, 2331, 2351, 2391, 2411, 2451, 2471, 2481, 2501, 2511) – and to_char(occurred_at,‘yyyy-mm’) like ‘2016-05’ – order by occurred_at desc\nQ4. How many sales reps represent the Northeast Region?\nselect region_id,count(id) from sqlchallenge1.sales_reps group by region_id;\nselect * from sqlchallenge1.region\nregion_id count 3 10 4 10 1 21 2 9\nid name 1 Northeast 2 Midwest 3 Southeast 4 West\n============================================================\nSQL Aggregate Functions:\nHere’s a quick preview:\nCOUNT counts how many rows are in a particular column. SUM adds together all the values in a particular column. MIN and MAX return the lowest and highest values in a particular column, respectively. AVG calculates the average of a group of selected values.\nIntermediate SQL Putting it together SQL Aggregate Functions SQL COUNT SQL SUM SQL MIN/MAX SQL AVG SQL GROUP BY SQL HAVING SQL CASE SQL DISTINCT SQL Joins SQL INNER JOIN SQL Outer Joins SQL LEFT JOIN SQL RIGHT JOIN SQL Joins Using WHERE or ON SQL FULL OUTER JOIN SQL UNION SQL Joins with Comparison Operators SQL Joins on Multiple Keys SQL Self Joins\nSELECT COUNT(*) FROM tutorial.aapl_historical_stock_price\nNote: Typing COUNT(1) has the same effect as COUNT(*). Which one you use is a matter of personal preference.\nThe following code will provide a count of all of rows in which the high column is not null. COUNT cannot count NULL i.e. will not include NULL rows in the count.\nSELECT COUNT(high) FROM tutorial.aapl_historical_stock_price\nYou’ll notice that this result is lower than what you got with COUNT(*). That’s because high has some NULLs. In this case, we’ve deleted some data to make the lesson interesting, but analysts often run into naturally-occurring null rows.\n===========================\nSELECT COUNT(date) AS count_of_date FROM tutorial.aapl_historical_stock_price\nIf you must use spaces, you will need to use double quotes.\nSELECT COUNT(date) AS “Count Of Date” FROM tutorial.aapl_historical_stock_price\nNote: This is really the only place in which you’ll ever want to use double quotes in SQL. Single quotes for everything else.\n– In pgsql, double quotes for column names and derived column names. Everything else can be single quotes including strings.\n– Write a query that determines counts of every single column. With these counts, can you tell which column has the most null values?\nSELECT COUNT(date) as date, count(year) as year,count(month) as month, count(open) as open,count(high) as high, count(low) as low,count(close) as close, count(volume) as volume, count(id) as id FROM tutorial.aapl_historical_stock_price limit 1\n=========================================================================\nThe SQL SUM function\nSUM is a SQL aggregate function. that totals the values in a given column. Unlike COUNT, you can only use SUM on columns containing numerical values.\nThe query below selects the sum of the volume column from the Apple stock prices dataset:\nSELECT SUM(volume) FROM tutorial.aapl_historical_stock_price\nAn important thing to remember: aggregators only aggregate vertically. If you want to perform a calculation across rows, you would do this with simple arithmetic.\nYou don’t need to worry as much about the presence of nulls with SUM as you would with COUNT, as SUM treats nulls as 0. i.e. COUNT skips NULLs but SUM treats them as 0.\n– Write a query to calculate the average opening price (hint: you will need to use both COUNT and SUM, as well as some simple arithmetic.).\nSELECT sum(open)/count(open),sum(open)/count(1) FROM tutorial.aapl_historical_stock_price limit 1\n– This says that sum(open)/count(open) is right but there are events where there is no opening price? so should not sum(open)/count(1) be proper?\nThe SQL MIN and MAX functions\nMIN and MAX are SQL aggregation functions that return the lowest and highest values in a particular column.\nThey’re similar to COUNT in that they can be used on non-numerical columns. Depending on the column type, MIN will return the lowest number, earliest date, or non-numerical value as close alphabetically to “A” as possible. As you might suspect, MAX does the opposite—it returns the highest number, the latest date, or the non-numerical value closest alphabetically to “Z.”\n– &gt; COUNT, MIN, MAX can be used in non-numerical columns.\nThe SQL AVG function\nAVG is a SQL aggregate function that calculates the average of a selected group of values. It’s very useful, but has some limitations. First, it can only be used on numerical columns. Second, it ignores nulls completely. You can see this by comparing these two queries of the Apple stock prices dataset:\nSELECT AVG(high) FROM tutorial.aapl_historical_stock_price WHERE high IS NOT NULL\nThe above query produces the same result as the following query:\nSELECT AVG(high) FROM tutorial.aapl_historical_stock_price\nThere are some cases in which you’ll want to treat null values as 0. For these cases, you’ll want to write a statement that changes the nulls to 0 (covered in a later lesson.\n??? Make examples of all Aggregate functions with and without NULL condition and check.\nThe SQL GROUP BY clause\nSQL aggregate function like COUNT, AVG, and SUM have something in common: they all aggregate across the entire table. But what if you want to aggregate only part of a table? For example, you might want to count the number of entries for each year.\nIn situations like this, you’d need to use the GROUP BY clause. GROUP BY allows you to separate data into groups, which can be aggregated independently of one another. Here’s an example using the Apple stock prices dataset:\nSELECT year, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY year\n(You can group by multiple columns, but you have to separate column names with commas—just as with ORDER BY):\nSELECT year, month, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY year, month"
  },
  {
    "objectID": "sql-mode.html#practice-problem",
    "href": "sql-mode.html#practice-problem",
    "title": "INNER JOIN",
    "section": "Practice Problem",
    "text": "Practice Problem\nCalculate the total number of shares traded each month. Order your results chronologically. Try it out See the answer GROUP BY column numbers\nAs with ORDER BY, you can substitute numbers for column names in the GROUP BY clause. It’s generally recommended to do this only when you’re grouping many columns, or if something else is causing the text in the GROUP BY clause to be excessively long:\nSELECT year, month, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY 1, 2\nNote: this functionality (numbering columns instead of using names) is supported by Mode, but not by every flavor of SQL, so if you’re using another system or connected to certain types of databases, it may not work.\n– Using GROUP BY with ORDER BY\n\n\nThe order of column names in your GROUP BY clause doesn’t matter—the results will be the same regardless. If you want to control how the aggregations are grouped together, use ORDER BY.\n\n\n– &gt; Try running the query below, then reverse the column names in the ORDER BY statement and see how it looks:\nSELECT year, month, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY year, month ORDER BY month, year\nyear month count 2000 1 20 2001 1 21 2002 1 21 2003 1 21\nSELECT year, month, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY year, month ORDER BY year, month\nyear month count 2000 1 20 2000 2 20 2000 3 23 2000 4 19\n– GROUP BY column numbers\nAs with ORDER BY, you can substitute numbers for column names in the GROUP BY clause. It’s generally recommended to do this only when you’re grouping many columns, or if something else is causing the text in the GROUP BY clause to be excessively long:\nSELECT year, month, COUNT(*) AS count FROM tutorial.aapl_historical_stock_price GROUP BY 1, 2\nNote: this functionality (numbering columns instead of using names) is supported by Mode, but not by every flavor of SQL, so if you’re using another system or connected to certain types of databases, it may not work.\n– Using GROUP BY with LIMIT\nThere’s one thing to be aware of as you group by multiple columns: SQL evaluates the aggregations before the LIMIT clause. If you don’t group by any columns, you’ll get a 1-row result—no problem there. If you group by a column with enough unique values that it exceeds the LIMIT number, the aggregates will be calculated, and then some rows will simply be omitted from the results.\nThis is actually a nice way to do things because you know you’re going to get the correct aggregates. If SQL cuts the table down to 100 rows, then performed the aggregations, your results would be substantially different. The above query’s results exceed 100 rows, so it’s a perfect example. Try removing the limit and running it again to see what changes.\n– &gt;&gt; LIMIT only limitst the final results and not the aggregrate count. So it is useful for quickly scanning the top rows of the results to check for correctness.\n– The SQL HAVING clause\nIn the previous lesson, you learned how to use the GROUP BY clause to aggregate stats from the Apple stock prices dataset by month and year.\nHowever, you’ll often encounter datasets where GROUP BY isn’t enough to get what you’re looking for. Let’s say that it’s not enough just to know aggregated stats by month. After all, there are a lot of months in this dataset. Instead, you might want to find every month during which AAPL stock worked its way over $400/share. The WHERE clause won’t work for this because it doesn’t allow you to filter on aggregate columns—that’s where the HAVING clause comes in:\nSELECT year, month, MAX(high) AS month_high FROM tutorial.aapl_historical_stock_price GROUP BY year, month HAVING MAX(high) &gt; 400 ORDER BY year, month\nNote: HAVING is the “clean” way to filter a query that has been aggregated, but this is also commonly done using a subquery, which you will learn about in a later lesson. Query clause order\n– &gt;&gt; ?? Need to learn the difference between when to use HAVING and WHERE in aggregated columns.\nAs mentioned in prior lessons, the order in which you write the clauses is important. Here’s the order for everything you’ve learned so far:\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nORDER BY\n– The SQL CASE statement\nThe CASE statement is SQL’s way of handling if/then logic. The CASE statement is followed by at least one pair of WHEN and THEN statements—SQL’s equivalent of IF/THEN in Excel. Because of this pairing, you might be tempted to call this SQL CASE WHEN, but CASE is the accepted term.\nEvery CASE statement must end with the END statement. The ELSE statement is optional, and provides a way to capture values not specified in the WHEN/THEN statements. CASE is easiest to understand in the context of an example:\nSELECT player_name, year, CASE WHEN year = ‘SR’ THEN ‘yes’ ELSE NULL END AS is_a_senior FROM benn.college_football_players\nIn plain English, here’s what’s happening:\nThe CASE statement checks each row to see if the conditional statement—year = 'SR' is true.\nFor any given row, if that conditional statement is true, the word \"yes\" gets printed in the column that we have named is_a_senior.\nIn any row for which the conditional statement is false, nothing happens in that row, leaving a null value in the is_a_senior column.\nAt the same time all this is happening, SQL is retrieving and displaying all the values in the player_name and year columns.\nThe above query makes it pretty easy to see what’s happening because we’ve included the CASE statement along with the year column itself. You can check each row to see whether year meets the condition year = ‘SR’ and then see the result in the column generated using the CASE statement.\nBut what if you don’t want null values in the is_a_senior column? The following query replaces those nulls with “no”:\nSELECT player_name, year, CASE WHEN year = ‘SR’ THEN ‘yes’ ELSE ‘no’ END AS is_a_senior FROM benn.college_football_players\n– Practice Problem\n– Write a query that includes a column that is flagged “yes” when a player is from California, and sort the results with those players first.\nSELECT player_name, state, CASE WHEN state = ‘CA’ THEN ‘yes’ ELSE NULL END AS from_california FROM benn.college_football_players ORDER BY 3\nAdding multiple conditions to a CASE statement\nYou can also define a number of outcomes in a CASE statement by including as many WHEN/THEN statements as you’d like:\nSELECT player_name, weight, CASE WHEN weight &gt; 250 THEN ‘over 250’ WHEN weight &gt; 200 THEN ‘201-250’ WHEN weight &gt; 175 THEN ‘176-200’ ELSE ‘175 or under’ END AS weight_group FROM benn.college_football_players\nIn the above example, the WHEN/THEN statements will get evaluated in the order that they’re written. So if the value in the weight column of a given row is 300, it will produce a result of “over 250.” Here’s what happens if the value in the weight column is 180, SQL will do the following:\nCheck to see if weight is greater than 250. 180 is not greater than 250, so move on to the next WHEN/THEN\nCheck to see if weight is greater than 200. 180 is not greater than 200, so move on to the next WHEN/THEN\nCheck to see if weight is greater than 175. 180 is greater than 175, so record \"175-200\" in the weight_group column.\nWhile the above works, it’s really best practice to create statements that don’t overlap. WHEN weight &gt; 250 and WHEN weight &gt; 200 overlap for every value greater than 250, which is a little confusing. A better way to write the above would be:\nSELECT player_name, weight, CASE WHEN weight &gt; 250 THEN ‘over 250’ WHEN weight &gt; 200 AND weight &lt;= 250 THEN ‘201-250’ WHEN weight &gt; 175 AND weight &lt;= 200 THEN ‘176-200’ ELSE ‘175 or under’ END AS weight_group FROM benn.college_football_players\n– gives error that in count(poss), poss is not recognized. SELECT CASE when position = ‘?’ then ‘Question’ when position like ‘D%’ then ‘D-Type’ when position like ‘C%’ then ‘C-Type’ else ‘Others’ end as poss, count(poss) FROM benn.college_football_players group by poss\n– works\nSELECT CASE when position = ‘?’ then ‘Question’ when position like ‘D%’ then ‘D-Type’ when position like ‘C%’ then ‘C-Type’ else ‘Others’ end as poss, count(1) FROM benn.college_football_players group by poss\n– Practice Problem\n– Write a query that includes players’ names and a column that classifies them into four categories based on height. Keep in mind that the answer we provide is only one of many possible answers, since you could divide players’ heights in many ways.\nSELECT player_name, height, CASE WHEN height &gt; 74 THEN ‘over 74’ WHEN height &gt; 72 AND height &lt;= 74 THEN ‘73-74’ WHEN height &gt; 70 AND height &lt;= 72 THEN ‘71-72’ ELSE ‘under 70’ END AS height_group FROM benn.college_football_players\n– attempted answer SELECT CASE when height &gt; 74 and height &lt;=100 then ‘75+’ WHEN height &gt; 72 and height &lt;=74 THEN ‘73-74’ WHEN height &gt; 70 and height &lt;=70 THEN ‘71-72’ WHEN height &gt; 40 and height &lt;=70 THEN ‘41-70’ ELSE ‘40 or under’ END AS height_group, count(1) FROM benn.college_football_players GROUP by 1 order by 1 desc\nYou can also string together multiple conditional statements with AND and OR the same way you might in a WHERE clause:\nSELECT player_name, CASE WHEN year = ‘FR’ AND position = ‘WR’ THEN ‘frosh_wr’ ELSE NULL END AS sample_case_statement FROM benn.college_football_players\nA quick review of CASE basics:\nThe CASE statement always goes in the SELECT clause\nCASE must include the following components: WHEN, THEN, and END. ELSE is an optional component.\nYou can make any conditional statement using any conditional operator (like WHERE ) between WHEN and THEN. This includes stringing together multiple conditional statements using AND and OR.\nYou can include multiple WHEN statements, as well as an ELSE statement to deal with any unaddressed conditions.\nPractice Problem\nWrite a query that selects all columns from benn.college_football_players and adds an additional column that displays the player’s name if that player is a junior or senior. Try it out See the answer\nSELECT *, CASE WHEN year IN (‘JR’, ‘SR’) THEN player_name ELSE NULL END AS upperclass_player_name FROM benn.college_football_players\n– Using CASE with aggregate functions\nCASE’s slightly more complicated and substantially more useful functionality comes from pairing it with aggregate functions. For example, let’s say you want to only count rows that fulfill a certain condition. Since COUNT ignores nulls, you could use a CASE statement to evaluate the condition and produce null or non-null values depending on the outcome:\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ ELSE ‘Not FR’ END AS year_group, COUNT(1) AS count FROM benn.college_football_players GROUP BY CASE WHEN year = ‘FR’ THEN ‘FR’ ELSE ‘Not FR’ END\n– above and below are the same.\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ ELSE ‘Not FR’ END AS year_group, COUNT(1) AS count FROM benn.college_football_players GROUP BY 1\nNow, you might be thinking “why wouldn’t I just use a WHERE clause to filter out the rows I don’t want to count?” You could do that—it would look like this:\nSELECT COUNT(1) AS fr_count FROM benn.college_football_players WHERE year = ‘FR’\nBut what if you also wanted to count a couple other conditions? Using the WHERE clause only allows you to count one condition. Here’s an example of counting multiple conditions in one query:\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ WHEN year = ‘SO’ THEN ‘SO’ WHEN year = ‘JR’ THEN ‘JR’ WHEN year = ‘SR’ THEN ‘SR’ ELSE ‘No Year Data’ END AS year_group, COUNT(1) AS count FROM benn.college_football_players GROUP BY 1\nThe above query is an excellent place to use numbers instead of columns in the GROUP BY clause because repeating the CASE statement in the GROUP BY clause would make the query obnoxiously long. Alternatively, you can use the column’s alias in the GROUP BY clause like this:\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ WHEN year = ‘SO’ THEN ‘SO’ WHEN year = ‘JR’ THEN ‘JR’ WHEN year = ‘SR’ THEN ‘SR’ ELSE ‘No Year Data’ END AS year_group, COUNT(1) AS count FROM benn.college_football_players GROUP BY year_group\nNote that if you do choose to repeat the entire CASE statement, you should remove the AS year_group column naming when you copy/paste into the GROUP BY clause:\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ WHEN year = ‘SO’ THEN ‘SO’ WHEN year = ‘JR’ THEN ‘JR’ WHEN year = ‘SR’ THEN ‘SR’ ELSE ‘No Year Data’ END AS year_group, COUNT(1) AS count FROM benn.college_football_players GROUP BY CASE WHEN year = ‘FR’ THEN ‘FR’ WHEN year = ‘SO’ THEN ‘SO’ WHEN year = ‘JR’ THEN ‘JR’ WHEN year = ‘SR’ THEN ‘SR’ ELSE ‘No Year Data’ END\nCombining CASE statements with aggregations can be tricky at first. It’s often helpful to write a query containing the CASE statement first and run it on its own. Using the previous example, you might first write:\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ WHEN year = ‘SO’ THEN ‘SO’ WHEN year = ‘JR’ THEN ‘JR’ WHEN year = ‘SR’ THEN ‘SR’ ELSE ‘No Year Data’ END AS year_group, * FROM benn.college_football_players\nThe above query will show all columns in the benn.college_football_players table, as well as a column showing the results of the CASE statement. From there, you can replace the * with an aggregation and add a GROUP BY clause. Try this process if you struggle with either of the following practice problems. Practice Problem\nWrite a query that counts the number of 300lb+ players for each of the following regions: West Coast (CA, OR, WA), Texas, and Other (everywhere else).\n– Write a query that counts the number of 300lb+ players for each of the following regions: – West Coast (CA, OR, WA), Texas, and Other (everywhere else).\nselect case when state in (‘CA’,‘OR’,‘WA’) then ‘West Coast’ when state = ‘TX’ then ‘Texas’ else ‘Other’ end as region_group, count(region_group) – it cannot find region_group error\n FROM benn.college_football_players\n group by 1\n \nTry it out See the answer\nselect case when state in (‘CA’,‘OR’,‘WA’) then ‘West Coast’ when state = ‘TX’ then ‘Texas’ else ‘Other’ end as region_group,\ncount(case when weight &gt; 300 then 1 else NULL end) as weight_group FROM benn.college_football_players group by region_group\n==========================================================================\nSELECT CASE WHEN state IN (‘CA’, ‘OR’, ‘WA’) THEN ‘West Coast’ WHEN state = ‘TX’ THEN ‘Texas’ ELSE ‘Other’ END AS arbitrary_regional_designation, COUNT(1) AS players FROM benn.college_football_players WHERE weight &gt;= 300 GROUP BY 1\nPractice Problem\n– Write a query that counts the number of 300lb+ players for each of the following regions: West Coast (CA, OR, WA), Texas, and Other (everywhere else).\nSELECT CASE WHEN state IN (‘CA’, ‘OR’, ‘WA’) THEN ‘West Coast’ WHEN state = ‘TX’ THEN ‘Texas’ ELSE ‘Other’ END AS arbitrary_regional_designation, COUNT(1) AS players FROM benn.college_football_players WHERE weight &gt;= 300 GROUP BY 1\nPractice Problem\n– Write a query that calculates the combined weight of all underclass players (FR/SO) in California as well as the combined weight of all upperclass players (JR/SR) in California.\nSELECT CASE WHEN year IN (‘FR’, ‘SO’) THEN ‘underclass’ WHEN year IN (‘JR’, ‘SR’) THEN ‘upperclass’ ELSE NULL END AS class_group, SUM(weight) AS combined_player_weight FROM benn.college_football_players WHERE state = ‘CA’ GROUP BY 1\n– Using CASE inside of aggregate functions\nIn the previous examples, data was displayed vertically, but in some instances, you might want to show data horizontally. This is known as “pivoting” (like a pivot table in Excel). Let’s take the following query:\nSELECT CASE WHEN year = ‘FR’ THEN ‘FR’ WHEN year = ‘SO’ THEN ‘SO’ WHEN year = ‘JR’ THEN ‘JR’ WHEN year = ‘SR’ THEN ‘SR’ ELSE ‘No Year Data’ END AS year_group, COUNT(1) AS count FROM benn.college_football_players GROUP BY 1\nAnd re-orient it horizontally:\nSELECT COUNT(CASE WHEN year = ‘FR’ THEN 1 ELSE NULL END) AS fr_count, COUNT(CASE WHEN year = ‘SO’ THEN 1 ELSE NULL END) AS so_count, COUNT(CASE WHEN year = ‘JR’ THEN 1 ELSE NULL END) AS jr_count, COUNT(CASE WHEN year = ‘SR’ THEN 1 ELSE NULL END) AS sr_count FROM benn.college_football_players\nIt’s worth noting that going from horizontal to vertical orientation can be a substantially more difficult problem depending on the circumstances, and is covered in greater depth in a later lesson.\nSharpen your SQL skills – Practice Problem\nWrite a query that displays the number of players in each state, with FR, SO, JR, and SR players in separate columns and another column for the total number of players. Order results such that states with the most players come first. Try it out See the answer\n– SELECT CASE WHEN year = ‘FR’ THEN ‘FR’ – WHEN year = ‘SO’ THEN ‘SO’ – WHEN year = ‘JR’ THEN ‘JR’ – WHEN year = ‘SR’ THEN ‘SR’ – ELSE ‘No Year Data’ END AS year_group, – COUNT(1) AS count – FROM benn.college_football_players – GROUP BY 1\n– horizontal\nSELECT COUNT(CASE WHEN year = ‘FR’ THEN 1 ELSE NULL END) AS fr_count, COUNT(CASE WHEN year = ‘SO’ THEN 1 ELSE NULL END) AS so_count, COUNT(CASE WHEN year = ‘JR’ THEN 1 ELSE NULL END) AS jr_count, COUNT(CASE WHEN year = ‘SR’ THEN 1 ELSE NULL END) AS sr_count, count(1) as total FROM benn.college_football_players\n– Proper Answer\nSELECT state, COUNT(CASE WHEN year = ‘FR’ THEN 1 ELSE NULL END) AS fr_count, COUNT(CASE WHEN year = ‘SO’ THEN 1 ELSE NULL END) AS so_count, COUNT(CASE WHEN year = ‘JR’ THEN 1 ELSE NULL END) AS jr_count, COUNT(CASE WHEN year = ‘SR’ THEN 1 ELSE NULL END) AS sr_count, COUNT(1) AS total_players FROM benn.college_football_players GROUP BY state ORDER BY total_players DESC\n– Practice Problem\nWrite a query that shows the number of players at schools with names that start with A through M, and the number at schools with names starting with N - Z. Try it out See the answer\nSELECT COUNT(CASE WHEN name &gt;= ‘a’ and name &lt;n THEN 1 ELSE NULL END) AS AtoM, COUNT(CASE WHEN name &gt;= ‘n’ THEN 1 ELSE NULL END) AS NtoZ FROM benn.college_football_players\n– correct answer\nSELECT CASE WHEN school_name &lt; ‘n’ THEN ‘A-M’ WHEN school_name &gt;= ‘n’ THEN ‘N-Z’ ELSE NULL END AS school_name_group, COUNT(1) AS players FROM benn.college_football_players GROUP BY 1\nNext Lesson\n– SQL DISTINCT\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nUsing SQL DISTINCT for viewing unique values\nUsing DISTINCT in aggregations\nDISTINCT performance\nPractice problems\nUsing SQL DISTINCT for viewing unique values\nYou’ll occasionally want to look at only the unique values in a particular column. You can do this using SELECT DISTINCT syntax. To select unique values from the month column in the Apple stock prices dataset, you’d use the following query:\nSELECT DISTINCT month FROM tutorial.aapl_historical_stock_price\nIf you include two (or more) columns in a SELECT DISTINCT clause, your results will contain all of the unique pairs of those two columns:\nSELECT DISTINCT year, month FROM tutorial.aapl_historical_stock_price\nNote: You only need to include DISTINCT once in your SELECT clause—you do not need to add it for each column name. Practice Problem\n– Write a query that returns the unique values in the year column, in chronological order. Try it out See the answer\nSELECT DISTINCT year FROM tutorial.aapl_historical_stock_price order by year\nDISTINCT can be particularly helpful when exploring a new data set. In many real-world scenarios, you will generally end up writing several preliminary queries in order to figure out the best approach to answering your initial question. Looking at the unique values on each column can help identify how you might want to group or filter the data. Using DISTINCT in aggregations\nYou can use DISTINCT when performing an aggregation. You’ll probably use it most commonly with the COUNT function.\nIn this case, you should run the query below that counts the unique values in the month column.\nSELECT COUNT(DISTINCT month) AS unique_months FROM tutorial.aapl_historical_stock_price\nThe results show that there are 12 unique values (other examples may be less obvious). That’s a small enough number that you might be able to aggregate by month and interpret the results fairly early. For example, you might follow this up by taking average trade volumes by month to get a sense of when Apple stock really moves:\nSELECT month, AVG(volume) AS avg_trade_volume FROM tutorial.aapl_historical_stock_price GROUP BY month ORDER BY 2 DESC\nOkay, back to DISTINCT. You’ll notice that DISTINCT goes inside the aggregate function rather than at the beginning of the SELECT clause. Of course, you can SUM or AVG the distinct values in a column, but there are fewer practical applications for them. For MAX and MIN, you probably shouldn’t ever use DISTINCT because the results will be the same as without DISTINCT, and the DISTINCT function will make your query substantially slower to return results. DISTINCT performance\nIt’s worth noting that using DISTINCT, particularly in aggregations, can slow your queries down quite a bit. We’ll cover this in greater depth in a later lesson. Sharpen your SQL skills Practice Problem\n– Write a query that counts the number of unique values in the month column for each year.\nSELECT year, COUNT(DISTINCT month) AS months_count FROM tutorial.aapl_historical_stock_price GROUP BY year ORDER BY year\nTry it out See the answer Practice Problem\n– Write a query that separately counts the number of unique values in the month column and the number of unique values in the year column. Try it out See the answer\nNext Lesson\nSQL Joins ORDER BY average_weight DESC\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nIntro to SQL joins: relational concepts\nThe anatomy of a join\nAliases in SQL\nJOIN and ON\nIntro to SQL joins: relational concepts\nUp to this point, we’ve only been working with one table at a time. The real power of SQL, however, comes from working with data from multiple tables at once. If you remember from a previous lesson, the tables you’ve been working with up to this point are all part of the same schema in a relational database. The term “relational database” refers to the fact that the tables within it “relate” to one another—they contain common identifiers that allow information from multiple tables to be combined easily.\nTo understand what joins are and why they are helpful, let’s think about Twitter.\nTwitter has to store a lot of data. Twitter could (hypothetically, of course) store its data in one big table in which each row represents one tweet. There could be one column for the content of each tweet, one for the time of the tweet, one for the person who tweeted it, and so on. It turns out, though, that identifying the person who tweeted is a little tricky. There’s a lot to a person’s Twitter identity—a username, a bio, followers, followees, and more. Twitter could store all of that data in a table like this:\nLet’s say, for the sake of argument, that Twitter did structure their data this way. Every time you tweet, Twitter creates a new row in its database, with information about you and the tweet.\nBut this creates a problem. When you update your bio, Twitter would have to change that information for every one of your tweets in this table. If you’ve tweeted 5,000 times, that means 5,000 changes. If many people on Twitter are making lots of changes at once, that’s a lot of computation to support. Instead, it’s much easier for Twitter to store everyone’s profile information in a separate table. That way, whenever someone updates their bio, Twitter would only have to change one row of data instead of thousands.\nIn an organization like this, Twitter now has two tables. The first table—the users table—contains profile information, and has one row per user. The second table—the tweets table—contains tweet information, including the username of the person who sent the tweet. By matching—or joining—that username in the tweets table to the username in the users table, Twitter can still connect profile information to every tweet. The anatomy of a join\nUnfortunately, we can’t use Twitter’s data in any working examples (for that, we’ll have to wait for the NSA’s SQL Tutorial), but we can look at a similar problem.\nIn the previous lesson on conditional logic, we worked with a table of data on college football players—benn.college_football_players. This table included data on players, including each player’s weight and the school that they played for. However, it didn’t include much information on the school, such as the conference the school is in—that information is in a separate table, benn.college_football_teams.\nLet’s say we want to figure out which conference has the highest average weight. Given that information is in two separate tables, how do you do that? A join!\nSELECT teams.conference AS conference, AVG(players.weight) AS average_weight FROM benn.college_football_players players JOIN benn.college_football_teams teams ON teams.school_name = players.school_name GROUP BY teams.conference ORDER BY AVG(players.weight) DESC\nand also order by average_weight desc\nThere’s a lot of new stuff happening here, so we’ll go step-by-step. Aliases in SQL\nWhen performing joins, it’s easiest to give your table names aliases. benn.college_football_players is pretty long and annoying to type—players is much easier. You can give a table an alias by adding a space after the table name and typing the intended name of the alias. As with column names, best practice here is to use all lowercase letters and underscores instead of spaces.\nOnce you’ve given a table an alias, you can refer to columns in that table in the SELECT clause using the alias name. For example, the first column selected in the above query is teams.conference. Because of the alias, this is equivalent to benn.college_football_teams.conference: we’re selecting the conference column in the college_football_teams table in benn’s schema.\nPractice Problem\n– Write a query that selects the school name, player name, position, and weight for every player in Georgia, ordered by weight (heaviest to lightest). Be sure to make an alias for the table, and to reference all column names in relation to the alias. Try it out See the answer\nSELECT players.school_name, players.player_name, players.position, players.weight FROM benn.college_football_players players WHERE players.state = ‘GA’ ORDER BY players.weight DESC\nJOIN and ON\nAfter the FROM statement, we have two new statements: JOIN, which is followed by a table name, and ON, which is followed by a couple column names separated by an equals sign.\nThough the ON statement comes after JOIN, it’s a bit easier to explain it first. ON indicates how the two tables (the one after the FROM and the one after the JOIN) relate to each other. You can see in the example above that both tables contain fields called school_name. Sometimes relational fields are slightly less obvious. For example, you might have a table called schools with a field called id, which could be joined against school_id in any other table. These relationships are sometimes called “mappings.” teams.school_name and players.school_name, the two columns that map to one another, are referred to as “foreign keys” or “join keys.” Their mapping is written as a conditional statement:\nON teams.school_name = players.school_name\nIn plain English, this means:\nJoin all rows from the players table on to rows in the teams table for which the school_name field in the players table is equal to the school_name field in the teams table.\nWhat does this actually do? Let’s take a look at one row to see what happens. This is the row in the players table for Wake Forest wide receiver Michael Campanaro:\nDuring the join, SQL looks up the school_name—in this case, “Wake Forest”—in the school_name field of the teams table. If there’s a match, SQL takes all five columns from the teams table and joins them to ten columns of the players table. The new result is a fifteen column table, and the row with Michael Campanaro looks like this:\nTwo columns are cut off from the image, but you can see the full result here.\nWhen you run a query with a join, SQL performs the same operation as it did above to every row of the table after the FROM statement. To see the full table returned by the join, try running this query:\nSELECT * FROM benn.college_football_players players JOIN benn.college_football_teams teams ON teams.school_name = players.school_name\nNote that SELECT * returns all of the columns from both tables, not just from the table after FROM. If you want to only return columns from one table, you can write SELECT players.* to return all the columns from the players table.\nOnce you’ve generated this new table after the join, you can use the same aggregate functions from a previous lesson. By running an AVG function on player weights, and grouping by the conference field from the teams table, you can figure out each conference’s average weight.\nSQL INNER JOIN\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nINNER JOIN\nJoining tables with identical column names\nPractice problem"
  },
  {
    "objectID": "sql-mode.html#sql-outer-joins",
    "href": "sql-mode.html#sql-outer-joins",
    "title": "INNER JOIN",
    "section": "SQL Outer Joins",
    "text": "SQL Outer Joins\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nOuter joins\nThe Crunchbase dataset\nOuter joins\nOuter joins are joins that return matched values and unmatched values from either or both tables. There are a few types of outer joins:\nLEFT JOIN returns only unmatched rows from the left table, as well as matched rows in both tables.\nRIGHT JOIN returns only unmatched rows from the right table , as well as matched rows in both tables.\nFULL OUTER JOIN returns unmatched rows from both tables,as well as matched rows in both tables.\nNote: LEFT JOIN is also refered to as OUTER LEFT JOIN. RIGHT JOIN is also refered to as OUTER RIGHT JOIN. FULL OUTER JOIN is also refered to as OUTER JOIN. Outer joins vs. Inner join\nWhen performing an inner join, rows from either table that are unmatched in the other table are not returned. In an outer join, unmatched rows in one or both tables can be returned.\nAs you work through the following lessons about outer joins, it might be helpful to refer to this JOIN visualization by Patrik Spathon.\nThe Crunchbase dataset\nThe data for the following lessons was pulled from Crunchbase, a crowdsourced index of startups, founders, investors, and the activities of all three. It was collected Feb. 5, 2014, and large portions of both tables were randomly dropped for the sake of this lesson. The first table lists a large portion of companies in the database; one row per company. The permalink field is a unique identifier for each row, and also shows the web address. For each company in the table, you can view its online Crunchbase profile by copying/pasting its permalink after Crunchbase’s web domain. For example, the third company in the table, “.Club Domains,” has the permalink “/company/club-domains,” so its profile address would be http://www.crunchbase.com/company/club-domains. The fields with “funding” in the name have to do with how much outside investment (in USD) each company has taken on. The rest of the fields are self-explanatory.\nSELECT * FROM tutorial.crunchbase_companies\nThe second table lists acquisitions—one row per acquisition. company_permalink in this table maps to the permalink field in tutorial.crunchbase_companies as described in the previous lesson. Joining these two fields will add information about the company being acquired.\nYou’ll notice that there is a separate field called acquirer_permalink as well. This can also be mapped to the permalink field tutorial.crunchbase_companies to add additional information about the acquiring company.\nSELECT * FROM tutorial.crunchbase_acquisitions\nThe foreign key you use to join these two tables will depend entirely on whether you’re looking to add information about the acquiring company or the company that was acquired.\nIt’s worth noting that this sort of structure is common. For example, a table showing a list of emails sent might include a sender_email_address and a recipient_email_address, both of which map to a table listing email addresses and the names of their owners.\nSQL LEFT JOIN\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nThe LEFT JOIN command\nPractice problems\nThe LEFT JOIN command {{ page.seo-title }}\nLet’s start by running an INNER JOIN on the Crunchbase dataset and taking a look at the results. We’ll just look at company-permalink in each table, as well as a couple other fields, to get a sense of what’s actually being joined.\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_companies companies JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink\nYou may notice that “280 North” appears twice in this list. That is because it has two entries in the tutorial.crunchbase_acquisitions table, both of which are being joined onto the tutorial.crunchbase_companies table.\nNow try running that query as a LEFT JOIN:\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink\nYou can see that the first two companies from the previous result set, #waywire and 1000memories, are pushed down the page by a number of results that contain null values in the acquisitions_permalink and acquired_date fields.\nThis is because the LEFT JOIN command tells the database to return all rows in the table in the FROM clause, regardless of whether or not they have matches in the table in the LEFT JOIN clause. Sharpen your SQL skills\nYou can explore the differences between a LEFT JOIN and a JOIN by solving these practice problems:\nPractice Problem\n– Write a query that performs an inner join between the tutorial.crunchbase_acquisitions table and the tutorial.crunchbase_companies table, but instead of listing individual rows, count the number of non-null rows in each table. Try it out See the answer Practice Problem\n– Modify the query above to be a LEFT JOIN. Note the difference in results. Try it out See the answer\nSELECT COUNT(companies.permalink) AS companies_rowcount, COUNT(acquisitions.company_permalink) AS acquisitions_rowcount FROM tutorial.crunchbase_companies companies JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink\nNow that you’ve got a sense of how left joins work, try this harder aggregation problem:\nPractice Problem\n– Count the number of unique companies (don’t double-count companies) and unique acquired companies by state. Do not include results for which there is no state data, and order by the number of acquired companies from highest to lowest. Try it out See the answer\nSELECT COUNT(companies.permalink) AS companies_rowcount, COUNT(acquisitions.company_permalink) AS acquisitions_rowcount FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink\nSQL RIGHT JOIN\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nThe RIGHT JOIN command\nPractice problem\nThe RIGHT JOIN command\nRight joins are similar to left joins except they return all rows from the table in the RIGHT JOIN clause and only matching rows from the table in the FROM clause. SQL RIGHT JOIN Diagram\nRIGHT JOIN is rarely used because you can achieve the results of a RIGHT JOIN by simply switching the two joined table names in a LEFT JOIN. For example, in this query of the Crunchbase dataset, the LEFT JOIN section:\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink\nproduces the same results as this query:\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_acquisitions acquisitions RIGHT JOIN tutorial.crunchbase_companies companies ON companies.permalink = acquisitions.company_permalink\nThe convention of always using LEFT JOIN probably exists to make queries easier to read and audit, but beyond that there isn’t necessarily a strong reason to avoid using RIGHT JOIN.\nIt’s worth noting that LEFT JOIN and RIGHT JOIN can be written as LEFT OUTER JOIN and RIGHT OUTER JOIN, respectively.\nSharpen your SQL skills\nPractice Problem\n– Rewrite the previous practice query in which you counted total and acquired companies by state, but with a RIGHT JOIN instead of a LEFT JOIN. The goal is to produce the exact same results.\nSELECT COUNT(companies.permalink) AS companies_rowcount, COUNT(acquisitions.company_permalink) AS acquisitions_rowcount FROM tutorial.crunchbase_companies companies RIGHT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink\nTry it out See the answer\nNow that you’ve got a sense of how left joins work, try this harder aggregation problem:\nPractice Problem\n– Count the number of unique companies (don’t double-count companies) and unique acquired companies by state. Do not include results for which there is no state data, and order by the number of acquired companies from highest to lowest. Try it out See the answer\nSELECT companies.state_code, COUNT(DISTINCT companies.permalink) AS unique_companies, COUNT(DISTINCT acquisitions.company_permalink) AS unique_companies_acquired FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink WHERE companies.state_code IS NOT NULL GROUP BY 1 ORDER BY 3 DESC\nSQL Joins Using WHERE or ON\nStarting here? This lesson is part of a full-length tutorial in using SQL for Data Analysis. Check out the beginning.\nIn this lesson we’ll cover:\nFiltering in the ON clause\nFiltering in the WHERE clause\nPractice problems\nFiltering in the ON clause\nNormally, filtering is processed in the WHERE clause once the two tables have already been joined. It’s possible, though that you might want to filter one or both of the tables before joining them. For example, you only want to create matches between the tables under certain circumstances.\nUsing Crunchbase data, let’s take another look at the LEFT JOIN example from an earlier lesson (this time we’ll add an ORDER BY clause):\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink ORDER BY 1\nCompare the following query to the previous one and you will see that everything in the tutorial.crunchbase_acquisitions table was joined on except for the row for which company_permalink is ‘/company/1000memories’:\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink AND acquisitions.company_permalink != ‘/company/1000memories’ ORDER BY 1\nWhat’s happening above is that the conditional statement AND… is evaluated before the join occurs. You can think of it as a WHERE clause that only applies to one of the tables. You can tell that this is only happening in one of the tables because the 1000memories permalink is still displayed in the column that pulls from the other table: Filtering in the WHERE clause\nIf you move the same filter to the WHERE clause, you will notice that the filter happens after the tables are joined. The result is that the 1000memories row is joined onto the original table, but then it is filtered out entirely (in both tables) in the WHERE clause before displaying results.\nSELECT companies.permalink AS companies_permalink, companies.name AS companies_name, acquisitions.company_permalink AS acquisitions_permalink, acquisitions.acquired_at AS acquired_date FROM tutorial.crunchbase_companies companies LEFT JOIN tutorial.crunchbase_acquisitions acquisitions ON companies.permalink = acquisitions.company_permalink WHERE acquisitions.company_permalink != ‘/company/1000memories’ OR acquisitions.company_permalink IS NULL ORDER BY 1\nYou can see that the 1000memories line is not returned (it would have been between the two highlighted lines below). Also note that filtering in the WHERE clause can also filter null values, so we added an extra line to make sure to include the nulls. Sharpen your SQL skills\nFor this set of practice problems, we’re going to introduce a new dataset: tutorial.crunchbase_investments. This table is also sourced from Crunchbase and contains much of the same information as the tutorial.crunchbase_companies data. It it structured differently, though: it contains one row per investment. There can be multiple investments per company—it’s even possible that one investor could invest in the same company multiple times. The column names are pretty self-explanatory. What’s important is that company_permalink in the tutorial.crunchbase_investments table maps to permalink in the tutorial.crunchbase_companies table. Keep in mind that some random data has been removed from this table for the sake of this lesson.\nIt is very likely that you will need to do some exploratory analysis on this table to understand how you might solve the following problems.\nPractice Problem\n– Write a query that shows a company’s name, “status” (found in the Companies table), and the number of unique investors in that company. Order by the number of investors from most to fewest. Limit to only companies in the state of New York.\nTry it out See the answer Practice Problem\n– Write a query that lists investors based on the number of companies in which they are invested. Include a row for companies with no investor, and order from most companies to least. Try it out See the answer\n==================================\n– Show all tables in a particular Schema SELECT * FROM pg_catalog.pg_tables WHERE schemaname='tutorial'\nschemaname tablename tableowner tablespace hasindexes hasrules hastriggers rowsecurity tutorial animial_crossing_accessories mode_admin false false false false tutorial animial_crossing_accessories_9067b7cc3b49da87072a3ca5 mode_admin false false false false tutorial animal_crossing_achievements mode_admin false false false false tutorial animal_crossing_achievements_28e4e04e975e4f27cb9be9e1 mode_admin false false false false tutorial animal_crossing_art mode_admin false false false false tutorial animal_crossing_art_3a053143a5a8c3ae9b75d0ed mode_admin false false false false tutorial animal_crossing_bags mode_admin false false false false tutorial animal_crossing_bags_9de2101d615e87fd86a3e478 mode_admin false false false false tutorial animal_crossing_construction mode_admin false false false false tutorial animal_crossing_construction_8206ef829fca465fdd8701d5 mode_admin false false false false tutorial animal_crossing_bottoms mode_admin false false false false tutorial animal_crossing_bottoms_7b62ddba747aef4ef8c8d8a2 mode_admin false false false false tutorial animal_crossing_dress_up mode_admin false false false false tutorial animal_crossing_dress_up_ce7358a56aa5a9b8c2a306f2 mode_admin false false false false tutorial animal_crossing_fencing mode_admin false false false false tutorial animal_crossing_fencing_b9fa7af2f25249c4711acb51 mode_admin false false false false tutorial animal_crossing_floors mode_admin false false false false tutorial animal_crossing_floors_9a886f6549452cbf5efc25e7 mode_admin false false false false tutorial animal_crossing_fish mode_admin false false false false tutorial animal_crossing_fish_f65f35f1632978a4c5f990ef mode_admin false false false false tutorial animal_crossing_fossils mode_admin false false false false tutorial animal_crossing_fossils_2c9272e9e0b33df09cdc65d3 mode_admin false false false false tutorial animal_crossing_headwear mode_admin false false false false tutorial animal_crossing_headwear_ddba49991a05555e32792812 mode_admin false false false false tutorial animal_crossing_housewares mode_admin false false false false tutorial animal_crossing_housewares_c8ed0d98ee526b2ba1359fd5 mode_admin false false false false tutorial animal_crossing_insects mode_admin false false false false tutorial animal_crossing_insects_acc4b875f97d74102169d2ac mode_admin false false false false tutorial animal_crossing_miscellaneous mode_admin false false false false tutorial animal_crossing_miscellaneous_bdbfb596fce21c3f10335d1c mode_admin false false false false tutorial animal_crossing_music mode_admin false false false false tutorial animal_crossing_music_e7d082f93c66d3a22f746448 mode_admin false false false false tutorial animal_crossing_other mode_admin false false false false tutorial animal_crossing_other_349705c2bea7ac80e9aa1f8a mode_admin false false false false tutorial animal_crossing_posters mode_admin false false false false tutorial animal_crossing_posters_bae92ee5c32fb77373406dab mode_admin false false false false tutorial animal_crossing_recipes mode_admin false false false false tutorial animal_crossing_recipes_9db63044c47e79231fc13265 mode_admin false false false false tutorial animal_crossing_shoes mode_admin false false false false tutorial animal_crossing_shoes_3728971e61f59340c363023f mode_admin false false false false tutorial animal_crossing_rugs mode_admin false false false false tutorial animal_crossing_rugs_1c3f037f78650fd5246a6fdb mode_admin false false false false tutorial animal_crossing_tools mode_admin false false false false tutorial animal_crossing_tools_c5ae71342d66c5badbdfb82a mode_admin false false false false tutorial animal_crossing_tops mode_admin false false false false tutorial animal_crossing_tops_615ab87adf8163d8a3632515 mode_admin false false false false tutorial animal_crossing_umbrellas mode_admin false false false false tutorial animal_crossing_umbrellas_6b7b894344d22db6ba932f4e mode_admin false false false false tutorial animal_crossing_wallpaper mode_admin false false false false tutorial animal_crossing_wallpaper_690812972f7225866204afbd mode_admin false false false false tutorial animal_crossing_wall_mounted mode_admin false false false false tutorial animal_crossing_wall_mounted_2ec1aae4ebae36d6b3015cea mode_admin false false false false tutorial animal_crossing_villagers mode_admin false false false false tutorial animal_crossing_villagers_3f286f728bf68dc64e41884c mode_admin false false false false tutorial animal_crossing_photos mode_admin false false false false tutorial animal_crossing_photos_ef96196614c1f8dc943eb969 mode_admin false false false false tutorial reactions mode_admin false false false false tutorial reactions_5b88e629c63295f5037e5dd7 mode_admin false false false false tutorial socks mode_admin false false false false tutorial socks_83b7a623aa1f412cf7c27191 mode_admin false false false false tutorial periodic_table mode_admin false false false false tutorial periodic_table_3da367b4eaee33483bb07aa2 mode_admin false false false false tutorial accounts mode_admin false false false false tutorial city_populations mode_admin true false false false tutorial aapl_historical_stock_price mode_admin true false false false tutorial aapl_historical_stock_price_729e68e4133568c5bd5b1fd6 mode_admin true false false false tutorial accounts_df8aa6d9a2a88b831c67e50d mode_admin false false false false tutorial city_populations_e5890d99441ced7db0e917c8 mode_admin true false false false tutorial crunchbase_acquisitions mode_admin true false false false tutorial crunchbase_acquisitions_clean_date mode_admin true false false false tutorial crunchbase_companies_23e8841677dd6b034ccf334a mode_admin true false false false tutorial crunchbase_acquisitions_clean_date_3de5ea9e69faa2d4c0daa50b mode_admin true false false false tutorial crunchbase_acquisitions_e6ea470d001f50dee95e911f mode_admin true false false false tutorial crunchbase_companies mode_admin true false false false tutorial crunchbase_companies_clean_date mode_admin true false false false tutorial crunchbase_companies_clean_date_bba7281543863b8c94439a02 mode_admin true false false false tutorial crunchbase_investments mode_admin true false false false tutorial crunchbase_investments_59edee3d798d4e482b9f9c27 mode_admin true false false false tutorial crunchbase_investments_part1 mode_admin true false false false tutorial crunchbase_investments_part1_35c1eed808839504576cc2b4 mode_admin true false false false tutorial crunchbase_investments_part2 mode_admin true false false false tutorial crunchbase_investments_part2_fc6a02204dc27b40b2fa9240 mode_admin true false false false tutorial excel_sql_inventory_data mode_admin false false false false tutorial dc_bikeshare_q1_2012 mode_admin true false false false tutorial dunder_mifflin_paper_sales mode_admin false false false false tutorial dc_bikeshare_q1_2012_5b21eebb3d1f611898541dd8 mode_admin true false false false tutorial dunder_mifflin_paper_sales_e84a724e74c938d968ab91fb mode_admin false false false false tutorial excel_sql_transaction_data_4cf18c59e5ecd52e57169740 mode_admin false false false false tutorial excel_sql_inventory_data_1ee683fdce61d5baec32b00e mode_admin false false false false tutorial excel_sql_transaction_data mode_admin false false false false tutorial flight_revenue_d092d80fd70090b4731ec31d mode_admin true false false false tutorial flight_revenue mode_admin true false false false tutorial flights mode_admin false false false false tutorial kag_conversion_data mode_admin false false false false tutorial flights_5f419315c91e82d60e8e0875 mode_admin false false false false tutorial global_weekly_charts_2013_2014 mode_admin true false false false tutorial nominee_information mode_admin true false false false tutorial global_weekly_charts_2013_2014_eb98d24259d4a9241abc9e9a mode_admin true false false false tutorial housing_units_completed_us mode_admin false false false false tutorial housing_units_completed_us_6887847a8865b7e27ded2c5d mode_admin false false false false tutorial nominee_filmography_79d0230f7e75cc81d4cee943 mode_admin true false false false tutorial kag_conversion_data_2c2f81a8d32497b0415e4a15 mode_admin false false false false tutorial nominee_filmography mode_admin true false false false tutorial nominee_information_1aabb7c60ed4a218e99ef3ff mode_admin true false false false tutorial olist_closed_deals_dataset mode_admin false false false false tutorial olist_closed_deals_dataset_aa7c34b34738a8d544480807 mode_admin false false false false tutorial olist_marketing_qualified_leads_dataset mode_admin false false false false tutorial olist_marketing_qualified_leads_dataset_9c092ba603934d607fbc10a mode_admin false false false false tutorial orders mode_admin false false false false tutorial orders_8bb73dd06ead6bc5f0e29032 mode_admin false false false false tutorial oscar_nominees mode_admin true false false false tutorial patient_list mode_admin true false false false tutorial oscar_nominees_4d120ddc8a5d160d382080c4 mode_admin true false false false tutorial playbook_emails_0e0407ff792c147582413c79 mode_admin false false false false tutorial patient_list_0a64d48a4d61ee3157779028 mode_admin true false false false tutorial playbook_emails mode_admin false false false false tutorial playbook_events_9b1b64883991067bd829c07e mode_admin false false false false tutorial playbook_events mode_admin false false false false tutorial playbook_experiments mode_admin false false false false tutorial playbook_experiments_308636a76661de422e58112a mode_admin false false false false tutorial sales_reps mode_admin false false false false tutorial playbook_users mode_admin false false false false tutorial sales_performance mode_admin true false false false tutorial playbook_users_b855ac7269b53346b63023a5 mode_admin false false false false tutorial regions mode_admin false false false false tutorial regions_ac8db41348e3e07ea8ecfa3b mode_admin false false false false tutorial sales_performance_98a4f8fe80ec93fa51d3e6ac mode_admin true false false false tutorial sat_scores mode_admin true false false false tutorial sales_reps_955282f7425fd8aa35d63e2c mode_admin false false false false tutorial sat_scores_dee2756beb74f1d54f19ae0e mode_admin true false false false tutorial sf_crime_incidents_2014_01 mode_admin true false false false tutorial sf_crime_incidents_2014_01_b4e0bc1587c3b5d55dfe4147 mode_admin true false false false tutorial test_table_1245034f7b728e33b379cae5 mode_admin true false false false tutorial sf_crime_incidents_cleandate mode_admin true false false false tutorial sf_crime_incidents_cleandate_63d6ccfbee85a9b2d199cc4e mode_admin true false false false tutorial test_table mode_admin true false false false tutorial us_housing_units_completed_9fd74772e020626793e02431 mode_admin true false false false tutorial us_housing_units_1309f10be77d5233cc1faaf4 mode_admin false false false false tutorial us_flights mode_admin false false false false tutorial us_flights_59b24e427c9c8acac6d3da53 mode_admin false false false false tutorial us_housing_units mode_admin false false false false tutorial us_housing_units_completed mode_admin true false false false tutorial wa_fn_usec_telco_customer_churn_wa_fn_u_2adca2f6cfc10754235509d mode_admin false false false false tutorial wa_fn_usec_telco_customer_churn_wa_fn_usec_telco_customer_churn mode_admin false false false false tutorial watsi_events mode_admin false false false false tutorial watsi_events_677512b5b01f5e376e5a8526 mode_admin false false false false tutorial yammer_emails mode_admin false false false false tutorial worldwide_earthquakes mode_admin true false false false tutorial worldwide_earthquakes_a0a12bad0edf50224fe97c5c mode_admin true false false false tutorial yammer_emails_d96cf1b9f9e98c8761af2e54 mode_admin false false false false tutorial yammer_experiments mode_admin false false false false tutorial yammer_events mode_admin false false false false tutorial yammer_events_29ce87bdef256e1f540bfe90 mode_admin false false false false tutorial yammer_experiments_326f368ea19b7bbc7c7ad6a2 mode_admin false false false false tutorial yammer_users_ec7fb2789bca2e2ebf4d4e91 mode_admin false false false false tutorial yammer_users mode_admin false false false false tutorial billboard_top_100_year_end mode_admin false false false false tutorial billboard_top_100_year_end_8d3506ab0b609d56b1bd3a54 mode_admin false false false false tutorial animal_crossing_headwear mode_admin false false false false tutorial animal_crossing_headwear_ddba49991a05555e32792812 mode_admin false false false false tutorial animal_crossing_housewares mode_admin false false false false tutorial animal_crossing_housewares_c8ed0d98ee526b2ba1359fd5 mode_admin false false false false tutorial animal_crossing_insects mode_admin false false false false tutorial animal_crossing_insects_acc4b875f97d74102169d2ac mode_admin false false false false tutorial animal_crossing_miscellaneous mode_admin false false false false tutorial animal_crossing_miscellaneous_bdbfb596fce21c3f10335d1c mode_admin false false false false tutorial animal_crossing_music mode_admin false false false false tutorial animal_crossing_music_e7d082f93c66d3a22f746448 mode_admin false false false false tutorial animal_crossing_other mode_admin false false false false tutorial animal_crossing_other_349705c2bea7ac80e9aa1f8a mode_admin false false false false tutorial animal_crossing_posters mode_admin false false false false tutorial animal_crossing_posters_bae92ee5c32fb77373406dab mode_admin false false false false tutorial animal_crossing_recipes mode_admin false false false false tutorial animal_crossing_recipes_9db63044c47e79231fc13265 mode_admin false false false false tutorial animal_crossing_shoes mode_admin false false false false tutorial animal_crossing_shoes_3728971e61f59340c363023f mode_admin false false false false tutorial animal_crossing_rugs mode_admin false false false false tutorial animal_crossing_rugs_1c3f037f78650fd5246a6fdb mode_admin false false false false tutorial animal_crossing_tools mode_admin false false false false tutorial animal_crossing_tools_c5ae71342d66c5badbdfb82a mode_admin false false false false tutorial animal_crossing_tops mode_admin false false false false tutorial animal_crossing_tops_615ab87adf8163d8a3632515 mode_admin false false false false tutorial animal_crossing_umbrellas mode_admin false false false false tutorial animal_crossing_umbrellas_6b7b894344d22db6ba932f4e mode_admin false false false false tutorial animal_crossing_wallpaper mode_admin false false false false tutorial animal_crossing_wallpaper_690812972f7225866204afbd mode_admin false false false false tutorial animal_crossing_wall_mounted mode_admin false false false false tutorial animal_crossing_wall_mounted_2ec1aae4ebae36d6b3015cea mode_admin false false false false tutorial animal_crossing_villagers mode_admin false false false false tutorial animal_crossing_villagers_3f286f728bf68dc64e41884c mode_admin false false false false tutorial animal_crossing_photos mode_admin false false false false tutorial animal_crossing_photos_ef96196614c1f8dc943eb969 mode_admin false false false false tutorial reactions mode_admin false false false false tutorial reactions_5b88e629c63295f5037e5dd7 mode_admin false false false false tutorial socks mode_admin false false false false tutorial socks_83b7a623aa1f412cf7c27191 mode_admin false false false false tutorial periodic_table mode_admin false false false false tutorial periodic_table_3da367b4eaee33483bb07aa2 mode_admin false false false false tutorial accounts mode_admin false false false false tutorial city_populations mode_admin true false false false tutorial aapl_historical_stock_price mode_admin true false false false tutorial aapl_historical_stock_price_729e68e4133568c5bd5b1fd6 mode_admin true false false false tutorial accounts_df8aa6d9a2a88b831c67e50d mode_admin false false false false tutorial city_populations_e5890d99441ced7db0e917c8 mode_admin true false false false tutorial crunchbase_acquisitions mode_admin true false false false tutorial crunchbase_acquisitions_clean_date mode_admin true false false false tutorial crunchbase_companies_23e8841677dd6b034ccf334a mode_admin true false false false tutorial crunchbase_acquisitions_clean_date_3de5ea9e69faa2d4c0daa50b mode_admin true false false false tutorial crunchbase_acquisitions_e6ea470d001f50dee95e911f mode_admin true false false false tutorial crunchbase_companies mode_admin true false false false tutorial crunchbase_companies_clean_date mode_admin true false false false tutorial crunchbase_companies_clean_date_bba7281543863b8c94439a02 mode_admin true false false false tutorial crunchbase_investments mode_admin true false false false tutorial crunchbase_investments_59edee3d798d4e482b9f9c27 mode_admin true false false false tutorial crunchbase_investments_part1 mode_admin true false false false tutorial crunchbase_investments_part1_35c1eed808839504576cc2b4 mode_admin true false false false tutorial crunchbase_investments_part2 mode_admin true false false false tutorial crunchbase_investments_part2_fc6a02204dc27b40b2fa9240 mode_admin true false false false tutorial excel_sql_inventory_data mode_admin false false false false tutorial dc_bikeshare_q1_2012 mode_admin true false false false tutorial dunder_mifflin_paper_sales mode_admin false false false false tutorial dc_bikeshare_q1_2012_5b21eebb3d1f611898541dd8 mode_admin true false false false tutorial dunder_mifflin_paper_sales_e84a724e74c938d968ab91fb mode_admin false false false false tutorial excel_sql_transaction_data_4cf18c59e5ecd52e57169740 mode_admin false false false false tutorial excel_sql_inventory_data_1ee683fdce61d5baec32b00e mode_admin false false false false tutorial excel_sql_transaction_data mode_admin false false false false tutorial flight_revenue_d092d80fd70090b4731ec31d mode_admin true false false false tutorial flight_revenue mode_admin true false false false tutorial flights mode_admin false false false false tutorial kag_conversion_data mode_admin false false false false tutorial flights_5f419315c91e82d60e8e0875 mode_admin false false false false tutorial global_weekly_charts_2013_2014 mode_admin true false false false tutorial nominee_information mode_admin true false false false tutorial global_weekly_charts_2013_2014_eb98d24259d4a9241abc9e9a mode_admin true false false false tutorial housing_units_completed_us mode_admin false false false false tutorial housing_units_completed_us_6887847a8865b7e27ded2c5d mode_admin false false false false tutorial nominee_filmography_79d0230f7e75cc81d4cee943 mode_admin true false false false tutorial kag_conversion_data_2c2f81a8d32497b0415e4a15 mode_admin false false false false tutorial nominee_filmography mode_admin true false false false tutorial nominee_information_1aabb7c60ed4a218e99ef3ff mode_admin true false false false tutorial olist_closed_deals_dataset mode_admin false false false false tutorial olist_closed_deals_dataset_aa7c34b34738a8d544480807 mode_admin false false false false tutorial olist_marketing_qualified_leads_dataset mode_admin false false false false tutorial olist_marketing_qualified_leads_dataset_9c092ba603934d607fbc10a mode_admin false false false false tutorial orders mode_admin false false false false tutorial orders_8bb73dd06ead6bc5f0e29032 mode_admin false false false false tutorial oscar_nominees mode_admin true false false false tutorial patient_list mode_admin true false false false tutorial oscar_nominees_4d120ddc8a5d160d382080c4 mode_admin true false false false tutorial playbook_emails_0e0407ff792c147582413c79 mode_admin false false false false tutorial patient_list_0a64d48a4d61ee3157779028 mode_admin true false false false tutorial playbook_emails mode_admin false false false false tutorial playbook_events_9b1b64883991067bd829c07e mode_admin false false false false tutorial playbook_events mode_admin false false false false tutorial playbook_experiments mode_admin false false false false tutorial playbook_experiments_308636a76661de422e58112a mode_admin false false false false tutorial sales_reps mode_admin false false false false tutorial playbook_users mode_admin false false false false tutorial sales_performance mode_admin true false false false tutorial playbook_users_b855ac7269b53346b63023a5 mode_admin false false false false tutorial regions mode_admin false false false false tutorial regions_ac8db41348e3e07ea8ecfa3b mode_admin false false false false tutorial sales_performance_98a4f8fe80ec93fa51d3e6ac mode_admin true false false false tutorial sat_scores mode_admin true false false false tutorial sales_reps_955282f7425fd8aa35d63e2c mode_admin false false false false tutorial sat_scores_dee2756beb74f1d54f19ae0e mode_admin true false false false tutorial sf_crime_incidents_2014_01 mode_admin true false false false tutorial sf_crime_incidents_2014_01_b4e0bc1587c3b5d55dfe4147 mode_admin true false false false tutorial test_table_1245034f7b728e33b379cae5 mode_admin true false false false tutorial sf_crime_incidents_cleandate mode_admin true false false false tutorial sf_crime_incidents_cleandate_63d6ccfbee85a9b2d199cc4e mode_admin true false false false tutorial test_table mode_admin true false false false tutorial us_housing_units_completed_9fd74772e020626793e02431 mode_admin true false false false tutorial us_housing_units_1309f10be77d5233cc1faaf4 mode_admin false false false false tutorial us_flights mode_admin false false false false tutorial us_flights_59b24e427c9c8acac6d3da53 mode_admin false false false false tutorial us_housing_units mode_admin false false false false tutorial us_housing_units_completed mode_admin true false false false tutorial wa_fn_usec_telco_customer_churn_wa_fn_u_2adca2f6cfc10754235509d mode_admin false false false false tutorial wa_fn_usec_telco_customer_churn_wa_fn_usec_telco_customer_churn mode_admin false false false false tutorial watsi_events mode_admin false false false false tutorial watsi_events_677512b5b01f5e376e5a8526 mode_admin false false false false tutorial yammer_emails mode_admin false false false false tutorial worldwide_earthquakes mode_admin true false false false tutorial worldwide_earthquakes_a0a12bad0edf50224fe97c5c mode_admin true false false false tutorial yammer_emails_d96cf1b9f9e98c8761af2e54 mode_admin false false false false tutorial yammer_experiments mode_admin false false false false tutorial yammer_events mode_admin false false false false tutorial yammer_events_29ce87bdef256e1f540bfe90 mode_admin false false false false tutorial yammer_experiments_326f368ea19b7bbc7c7ad6a2 mode_admin false false false false tutorial yammer_users_ec7fb2789bca2e2ebf4d4e91 mode_admin false false false false tutorial yammer_users mode_admin false false false false tutorial billboard_top_100_year_end mode_admin false false false false tutorial billboard_top_100_year_end_8d3506ab0b609d56b1bd3a54 mode_admin false false false false\n– Describe all the tables in a particular schema\nSELECT * FROM information_schema.columns\nWHERE table_schema = 'tutorial'\n   AND table_name   = 'us_housing_units'\ntable_catalog table_schema table_name column_name ordinal_position column_default is_nullable data_type character_maximum_length character_octet_length numeric_precision numeric_precision_radix numeric_scale datetime_precision interval_type interval_precision character_set_catalog character_set_schema character_set_name collation_catalog collation_schema collation_name domain_catalog domain_schema domain_name udt_catalog udt_schema udt_name scope_catalog scope_schema scope_name maximum_cardinality dtd_identifier is_self_referencing is_identity identity_generation identity_start identity_increment identity_maximum identity_minimum identity_cycle is_generated generation_expression is_updatable d5b78k6rg4etlv tutorial us_housing_units year 1 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 1 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units month 2 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 2 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units month_name 3 YES text 1073741824 d5b78k6rg4etlv pg_catalog text 3 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units south 4 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 4 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units west 5 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 5 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units midwest 6 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 6 NO NO NO NEVER YES d5b78k6rg4etlv tutorial us_housing_units northeast 7 YES double precision 53 2 d5b78k6rg4etlv pg_catalog float8 7 NO NO NO NEVER YES\n–"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Mastering myself",
    "section": "",
    "text": "This is a test page to experiment with Quarto features.\nWhat does the day look like?\nTrello: Making use of daily.\nRevise daily useful skills. Data Science, Analytics, ML.\nWhat main projects do you have?\nInor: Design, PM, Coding? OTT: Design, PM, Coding? Balkani: Analytics SuperStore: Analytics / scrum sakila DAC: BigQuery, Looker, Google Analytics pandas Tableau R Shiny\nBuild foundations about AI/ML & Product Management.\nTrello: For all tasks that come up in Distraction.\nJira: Full Projects\nSpreadsheets: Data Entry for projects & tasks\nSo Spreadsheets track hours of waste/productive time. Trello is a dump for tasks to be done and completed. Jira is for work oriented and learning about Jira github etc…\nStudy pattern:\nDon’t proceed further until you revise your mental fullness.\nSlow hard road out of hell.\nIf you can’t think deeply about the code or the problem. You need rest. Anxiety or burn out may be happening.\nRevise revise revise.\nIncrease attention span.\nYou are 44, don’t do any simple stuff documentation. Time is over. Only do complex articles and stuff.\nWork pattern:\n\nHow to use QMD?\n\nqmd is for documenting and running code you know would work already.\nipynb to qmd\nusing visual studio.\n\nSkills: VS Code\n\n\n\nHow to use this website:\nasdf a\n`http://localhost:3221/#category=analysis`\n`http://localhost:3221/#category=analysis`\n`http://localhost:3221/#category=analysis`\n\n\n\nTrello Embed\n\n  Trello Board\n\n\n\n\nTableau Embed 3\n\n\n            \n\n\n\nTableau Embed 2\n\n\n\n\n\n\n\n\n\n\n\nTableau Embed\n\n\n1 + 1\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\n\ns=pd.Series([1,3,5,np.nan,6,8])\nprint(s)\nprint(type(s))\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nI’ll just put another example of this.\n\ndates=pd.date_range('20141101',periods=6)\ndates\n\nDatetimeIndex(['2014-11-01', '2014-11-02', '2014-11-03', '2014-11-04',\n               '2014-11-05', '2014-11-06'],\n              dtype='datetime64[ns]', freq='D')"
  }
]